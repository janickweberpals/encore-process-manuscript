---
title: "A Process for the Emulation of Comparative Oncology Trials with Real-world Evidence (ENCORE)"

code-fold: false
echo: false
number-sections: false
warnings: false
messages: false

format: 
  docx:
    fig-cap-location: top
    tbl-cap-location: top
    reference-doc: custom-reference-doc.docx
  pdf:
    fig-cap-location: top
    tbl-cap-location: top
    
editor: visual
bibliography: references.bib
csl: journal-of-clinical-oncology.csl

filters:
  - docx-landscape.lua
---

```{r}
#| label: setup
#| include: false

library(here)
library(dplyr)
library(knitr)
library(gt)
library(readxl)
library(fs)
library(webshot2)
library(tibble)
library(ggplot2)
library(cowplot)
library(mice)
library(MatchThem)
library(cobalt)
library(gtExtras)
library(glue)
library(tidyr)
```

**Authors**: Janick Weberpals^1^, Kenneth L. Kehl^2^, Donna R. Rivera^3^, Pallavi Mishra-Kalyani^3^, Georg Hahn^1^, Priyanka Anand^1^, Yanina Natanzon^4^, Janet Espirito^5^, Nicholas J Robert^5^, Andrew J. Belli^6^, Ching-Kun Wang^6^, Sebastian Schneeweiss^1^, Shirley V. Wang^1^

[Author affiliations:]{.underline}

^1^ Division of Pharmacoepidemiology and Pharmacoeconomics, Department of Medicine, Brigham and Women's Hospital, Harvard Medical School, Boston, MA, USA

^2^ Dana-Farber Cancer Institute, Boston, MA, USA

^3^ Oncology Center of Excellence, US Food and Drug Administration, Silver Spring, MD, USA

^4^ ConcertAI, Cambridge, MA, USA

^5^ Ontada, Boston, MA, USA

^6^ COTA, Inc., New York, NY, USA

[**Correspondence:**]{.underline}

Shirley V. Wang, PhD

Division of Pharmacoepidemiology and Pharmacoeconomics,

Department of Medicine, Brigham and Women's Hospital, Harvard Medical School,

1620 Tremont Street, Suite 3030-R, Boston, MA 02120, USA

Phone: +1 617-278-0932

Fax: + 1 617-232-8602

Email: [SWANG1\@BWH.HARVARD.EDU](SWANG1@BWH.HARVARD.EDU)

[**Article type:**]{.underline} Review

[**Manuscript word count:**]{.underline} 4,131 words / 8,000 words

[**Abstract word count:**]{.underline} 248 words / 250 words

[**Tables:**]{.underline} 3

[**Figures:**]{.underline} 3

[**Supplementary material:**]{.underline} Supplementary figures and material

[**Short running title**]{.underline}: Emulation of Comparative Oncology Trials with Real-world Evidence (ENCORE)

[**Keywords:**]{.underline} Oncology, Real-World Evidence, Trial emulation, EHR

[**Funding Statement:**]{.underline} This project was supported by an FDA BAA contract with contract number 75F40122C00181.

[**Competing Interests Statement:**]{.underline} Dr. Weberpals is now an employee of AstraZeneca and owns stocks in AstraZeneca. Dr. Wang has consulted ad hoc for Exponent Inc. and MITRE a federally funded research center for the Centers for Medicare and Medicaid Services on unrelated work. Dr. Schneeweiss is participating in investigator-initiated grants to the Brigham and Women's Hospital from Boehringer Ingelheim, Takeda, and UCB unrelated to the topic of this study. He owns equity in Aetion Inc., a software manufacturer. He is an advisor to Temedica GmbH, a patient-oriented data generation company. His interests were declared, reviewed, and approved by the Brigham and Women's Hospital in accordance with their institutional compliance policies.

[**Data sharing statement:**]{.underline} No data was analyzed as part of this project.

[**Analytic code sharing statement:**]{.underline} Code to reproduce this manuscript, including all figures and tables, can be found at <https://github.com/janickweberpals/encore-process-manuscript>.

[***Proposed target journals:***]{.underline} *CPT =\> JCO CCI =\> ...*

*Manuscript last updated: `r Sys.time()`*

{{< pagebreak >}}

# Abstract {.unnumbered}

Real-world evidence (RWE) studies are increasingly used to complement evidence from randomized controlled trials (RCTs), contextualizing the effectiveness and safety of medical interventions as delivered in clinical practice. Advancements in the curation and accessibility of electronic health record data (EHR) have presented the opportunity to investigate disease domains such as oncology, where administrative healthcare claims databases alone are not fit-for-purpose. The RCT DUPLICATE initiative has previously enhanced understanding of when RWE studies can draw causal conclusions by emulating trials in the cardio-metabolic, renal, and pulmonary clinical areas. In this process paper, we present the Emulation of Comparative Oncology Trials with Real-world Evidence (ENCORE) project, which aims to extend this work to oncology. This expansion will emulate 12 RCTs in multiple EHR databases across four different cancer indications, including non-small cell lung cancer, breast cancer, colorectal cancer, and multiple myeloma. The objectives of this project are to develop state-of-the-art methodological approaches and to apply them in a demonstration project designed to create insights for guiding the use of RWE in oncology. The project will place a special emphasis on systematic evaluation of fitness of data in relation to the study design and statistical analysis for a particular research question, and pre-registration of study protocols prior to analysis. Agreement of treatment effect estimates between RCTs and their respective emulations will be assessed using pre-specified criteria. Through extensive sensitivity analyses benchmarked against RCT results, the ENCORE project will inform understanding of how measurement, design, and analytic decisions influence bias and validity in oncological RWE studies.

{{< pagebreak >}}

# Background

Randomized controlled trials (RCTs) have been the gold standard for establishing the efficacy and safety of medical products. With the advent of the 21^st^ Century Cures Act directive,[@RWEFDA] the Food and Drug Administration (FDA) established a framework to increasingly consider real-world evidence (RWE) generated from routine-care health data such as electronic health records (EHR) to evaluate and contextualize the comparative safety and effectiveness of novel cancer therapies.[@purpura2022role] Accounting for 21% of all approvals, oncology was the disease area with the most FDA drug approvals in 2023[@senior2024fresh], RWE has particularly important potential to complement evidence coming from RCTs in the field of precision oncology where potential use cases include the assessment of effectiveness in patient populations that are underrepresented in RCTs, the construction of external control arms in single-arm trials where active recruitment may not be feasible, or the discovery of biomarkers among pan-tumor populations that harbor specific genomic and immuno-pathological signatures.

However, the validity and transportability of results derived between RWE studies and RCTs can depend on many factors and frequently referenced limitations include missing data, small sample sizes, data discontinuity,[@Merola2022; @joshua2022longitudinal] adoption of changes in guidelines in real-world care and the inability to measure and emulate common eligibility criteria, prognostic factors, and standardized response assessments in real-world data (RWD).[@rider2024emulations] While there are already published examples of oncology trial emulations,[@rider2024emulations; @merola2023aetion; @merola2024calibrating] a systematic and scaled approach to emulate a diverse set of different oncology trials in multiple heterogeneous databases is necessary to gain confidence in the validity of RWE studies and to provide context as to which questions can be validly answered.

The RCT DUPLICATE initiative[@wang2023emulation] increased our understanding of when RWE studies can come to causal conclusions on treatment effects by benchmarking results against RCTs under the assumption that each RCT finding reflects a causal treatment effect. In settings where the RCT designs could be emulated well, RWE studies came to the same conclusions.[@heyard2024design] However, prior work from RCT-DUPLICATE has focused primarily on emulating trials in the cardio-metabolic, renal, and pulmonary clinical areas using claims databases.

The *Emulation of Comparative Oncology Trials with Real-world Evidence* (ENCORE) project [@encoreFDA] aims to extend this work to the field of oncology. Studies in oncology come with their own unique set of challenges which must be systematically explored and understood. Building on a process co-developed with the FDA through RCT DUPLICATE,[@wang2023emulation] this expansion to oncology will emulate 12 randomized clinical trials using multiple specialty oncology EHR data sources. The process will emphasize transparency and include documented data fitness assessment[@rivera2024oncology; @gatto2022structured] for each RWD source with respect to each trial emulation as well as extensive sensitivity analyses to assess robustness of findings.

The objectives of this project are to 1) develop state-of-the-art methodological approaches and 2) to apply them to create insights that may provide guidance on the potential use of RWE for regulatory science in oncology. To achieve these objectives, this demonstration project will systematically emulate 12 oncology trials across four cancers and assess the agreement of treatment effect estimates between RCTs and their respective emulations.

In this process paper, we describe the design and process for the selection of the 12 oncology RCTs, the assessment of the database quality and selection, protocol development and pre-registration, study design and statistical analysis, and pre-specified agreement metrics to evaluate the concordance between RCTs and emulations.

# Methods

A visual summary of the entire systematic process from trial selection to final results is provided in @fig-process.

## Trial selection

The focus of ENCORE is to maximize potential learnings on when RWE studies can or cannot yield similar results compared to RCTs. To that end, the emphasis of the project is on trials of therapies for the most common cancers for which there has been substantial therapeutic development in recent years. After careful review and exchange with clinical and regulatory experts, four cancer indications were identified including lung cancer, breast cancer, colorectal cancer and multiple myeloma. For each cancer type we aim to conduct three trial emulations using multiple databases accessible for the scope of this project (i.e., the total number of emulations will equal 12 trials x *n* databases which are found fit-for-purpose for each trial).

We used a semi-automated process for trial selection where the eligibility criteria are documented in CONSORT diagrams showing reasons for excluding RCTs for each cancer type. The search was conducted using the AACT database which is a publicly available relational database developed and maintained by the Clinical Trials Transformation Initiative (CTTI) that contains all information (protocol and result data elements) about every study registered on ClinicalTrials.gov.[@tasneem2012database] To identify eligible trials, we used a combined search query strategy of the National Library of Medicine (NLM)-controlled *MeSH* term and a free keyword search for the respective cancer indication in the *conditions*, *studies* and *detailed_descriptions* fields of each trial entry on ClinicalTrials.gov.

Eligible trials had to fulfill the following basic criteria:

-   Interventional

-   Randomized

-   Intervention model: parallel assignment

-   Industry-sponsored

-   Trial start in 2011 or later

-   Primary purpose was to study treatment effects

-   Overall survival must be one of the endpoints reported (either as hazard ratio or median overall survival time)

-   Recruitment status: ‘Completed’ or ‘Active, not recruiting’

-   Feasibility and clinical relevance

The rationale and operationalization of each criterion is listed in detail in @tbl-criteria. We will mainly consider pivotal interventional, randomized trials after 2011 because treatment guidelines among included cancer indications have undergone significant changes in recent years. Due to the rapid adoption of new breakthrough therapies in routine care, it is unlikely to find patients who may be still treated with outdated treatment regimens in current clinical practice. We also did not include trials with results published too recently. This decision was in order to allow for enough data and follow-up time accrual in the databases used for this project. Although there have been substantial methodological advancements to increase our understanding on the emulation and comparison of real-world progression-free survival (PFS) and objective response rates (ORR) to a RECISTv1.1[@eisenhauer2009new]-based PFS and ORR assessment in RCTs [@ThanCCR; @mckelvey2024evaluation], imaging-based evaluations still hold a level of granularity which may not be necessarily reflected in chart-abstracted assessments of a patient's progression in routine care.[@rwdRECIST; @rivera2022friends] In addition, the timing and cadence of intervals between progression assessments can differ between RCTs and routine care which may result in measurement error and bias.[@ackerman2024measurement] Given this, and the large number of other methodological challenges like missing data, small sample sizes, data discontinuity and rapidly changing guideline treatments, we focus on the emulation of overall survival (OS) as the endpoint of interest. Therefore, we include only trials that have reported overall survival (OS) as one of the pre-specified endpoints in the protocol.

While most trial eligibility criteria could be operationalized in an automated fashion, our final criterion was an assessment of emulation feasibility and clinical relevance. This criterion involved extensive human review. The critical points considered in this step include an initial feasibility assessment of the data fitness,[@rivera2022friends] including assessment of whether critical eligibility criteria (e.g., biomarker status) and prognostic factors (e.g., ECOG performance score) are measurable and whether preliminary sample size counts are reasonable. Lastly, trial candidates were ranked and shortlisted into primary and runner-up candidates based on their clinical and regulatory relevance.

A list of tentative, shortlisted primary candidates is presented in @tbl-rcts and the corresponding selection process is illustrated in the CONSORT diagrams (Supplementary Figures 1-4). Naturally, the majority of trials cover advanced or metastatic cancer populations because a large proportion of drug development efforts have focused on these settings in recent years. A key learning that we aim to foster with the shortlisted trials is to achieve a better understanding how different disease settings (early, late), line settings (\[neo\]adjuvant, first line, advanced lines of therapy), therapy protocols (monotherapy, combination therapy) and population characteristics (simple versus complex genetic or immunological signatures) can be emulated using RWD. If more thorough feasibility assessments suggest that the threat of bias from mis-measurement of key study parameters or residual confounding remains high, or that the sample size is not sufficient, then runner-up candidates will be considered instead.

## Databases

The ENCORE project will use data from four oncology-specific electronic health records (EHR)-derived data sources (in alphabetical order): ConcertAI, COTA, Flatiron Health, McKesson/Ontada. All available databases draw from a comprehensive national sample of patients with cancer in the US with detailed EHR-derived information on the information necessary to study medication effectiveness in oncology. A detailed description and methodology on how patients are sampled will be provided with each trial emulation protocol. For ENCORE, not all databases will be available for each cancer indication and the names of the databases will be blinded and referred to as ENCORE DataBase (EDB) 1, 2, 3 and 4 for the final reporting of results (the numbering does not coincide with the above order of mention of the databases). If more than one database is considered fit-for-purpose for a respective trial emulation, the best possible analytic model will be employed for each database separately and final treatment effect estimates will be pooled using a meta-analytic approach using fixed effects (weights reflect the inverse of the variance of each database study) and random effects (weights reflect the inverse of the variance of each database study plus an additional variance term that quantifies the assumed heterogeneity between databases) models.[@nikolakopoulou2014interpret]

<!--# maybe data partners have  1-2 sentences and/or publications to reference? -->

## Protocol development

For each shortlisted and selected RCT, a detailed protocol, pre-specifying key elements of the trial emulation, will be developed using the HARPER protocol template[@wang2022harmonized] which has been recommended for regulatory submissions of RWE studies,[@guidelinegeneral] and will be registered on ClinicalTrials.gov after careful review by a clinical and FDA regulatory expert panel. Following the target trial emulation framework, we will provide an explicit statement and rationale on how each element will be emulated including database selection, covariate measurement, operationalization of key eligibility criteria, study design, data analysis and causal contrasts of interest.[@hernan2022target; @hernan2016specifying] Since it is common that oncology RCTs update OS estimates periodically based on accrued follow-up time, the protocol will give a brief summary of each emulated RCT and specify which target OS estimates will be used to compare agreement metrics to (see @sec-agreement-metrics). All eligibility criteria will be extracted based on publications, publicly available protocols and statistical analysis plans of the selected RCT.

### Emulation feasibility

#### **Fit-for-purpose data**

Real-world data fitness and emulation feasibility for each shortlisted candidate trial will be assessed in multiple steps based on guidance of the oncology quality, characterization, and assessment of real-world data (Oncology QCARD) Initiative.[@rivera2022friends] The first step assesses if relevant variables like exposure/line of therapy, outcomes, and covariates are generally available, measured and operationalizable in routine-care. Since a vast majority of oncological RCTs in recent years have focused on selected, biomarker-defined populations, subtleties in measurement and operationalizability of specific biomarkers must be reflected to ensure a representative and large enough study population. For example, immunotherapies have significantly changed the cancer treatment landscape since the approval of the first PD-L1 inhibitor in 2015. With many trials that have followed thereafter, the operationalization of the expression of the PD-L1 biomarker in RCTs (e.g., as a percent staining, tumor proportion score or combined positive score) has also evolved since then and PD-L1 '*positivity'* may have different definitions across calendar years based on different cut-off values.

According to the *Structured Process to Identify Fit-For-Purpose Data* (SPIFD) framework[@gatto2022structured], the next step will outline tables that describe how eligibility criteria will be ascertained using a color-coded heatmap that will indicate the level of confidence on how well each criterion can be emulated in each selected database. As there are general eligibility criteria in oncology trials which either will not be possible to emulate (e.g., physician-assessed survival prognosis of xy months) or that are clinically not relevant for the emulation of the trial (e.g., male patients should be willing to use barrier contraception), the study team will decide on key eligibility criteria for the emulation of the trial.

We will additionally provide a definition on how exposures, outcomes and covariates are exactly defined and operationalized in each respective database. There will be special emphasis on how exposure, in context of their respective disease and line of therapy settings, and the OS outcome will be emulated. For all considered databases, the OS endpoint is typically a composite that is derived from different sources comprising EHR abstractions, social security death index, obituary and other linkages. Given that not all relevant sources that provide mortality data are synchronized and updated uniformly, sensitivity analyses with more conservative (i.e., earlier) censoring dates will be considered for each trial emulation to mitigate the potential impact of ghost-time bias.[@meyer2020open]

#### **Descriptives and data exploration**

Critical aspects when emulating oncology trials are the choice and estimation of the appropriate estimand of interest.[@rufibach2018] Particularly when emulating pivotal trials of paradigm-changing treatments, multiple aspects need to be considered such as the contemporaneity of the (historical) control cohort, the adoption rate of the novel intervention in routine care, the magnitude of the clinical treatment benefit and the rate in which (particularly patients in the control arm) discontinue or cross-over to the interventional treatment, which could lastly bias emulated treatment effects towards the null. To that end, comprehensive data explorations will be performed as part of the protocol development to contextualize these parameters and (if reported) draw comparisons to the emulated trial. Examples for such standard diagnostics are visualized in @fig-initiators. All exploratory analyses will be conducted blinded towards the outcome to avoid influencing study design and analytic choices.

The distribution of patient characteristics, stratified by exposure status, will be examined in Table 1's before and after applying eligibility criteria and contrasted with the distributions of patient characteristics of the original RCT. Initial propensity score matching or weighting methods will be applied to ensure that measured pre-exposure covariates can be balanced, exposure cohorts are conditionally exchangeable at baseline and resulting sample sizes are still sufficient after matching or weighting. At this stage, all exploratory analyses will be conducted blinded towards the outcome to not bias any study design and analytic choices based on known outcome information.

#### **Statistical power**

Causal analyses of observational data may not have the same pre-requisites in terms of formal hypothesis testing and statistical power than RCTs since the number of 'recruited' patients is given and cannot be influenced.[@hernan2022causal] For this project, however, the emulations in EHR data must have at least equal power to the relevant trial for interpretation of the pre-specified agreements metrics between RCT and RWD results. Since the main outcome of interest is defined as time to all-cause mortality (OS), the estimation of the statistical power is driven by the number of events rather than the number of patients. To assess if the overall number of events, unstratified by exposure, is sufficient such that a significant difference can be detected based on the original RCT-reported hazard ratio (HR), the statistical power (1-$\beta$) will be estimated using Schoenfeld's sample-size formula for the proportional-hazards regression model.[@schoenfeld1981asymptotic]

<!--# https://github.com/keaven/gsDesign/blob/master/R/nEvents.R#L22C12-L22C78 -->

## Agreement metrics {#sec-agreement-metrics}

To formally compare treatment effects between RCTs and their respective emulations, we will adapt the approach of the RCT-DUPLICATE project.[@franklin2020nonrandomized; @wang2023emulation] That is, for the primary endpoint of interest (HR and corresponding 95% confidence intervals), we will derive three qualitative agreement metrics: statistical agreement, estimate agreement and agreement based on the standardized mean difference (SMD). Examples are illustrated @tbl-metrics.

**Statistical significance agreement**: agreement between RCT and emulated trial treatment effect with regards to directionality and statistical significance.

**Estimate agreement**: agreement that the estimated RWE treatment effect is within the 95% CI of the RCT treatment effect estimate. Provided that for some emulations, the power of the RWE study may be larger than that of the original RCT, this could lead to situations where there is no regulatory agreement although the treatment effect estimates are highly overlapping but with the RCT estimate crossing the null (or vice versa in case the RCT has a larger power than the RWE emulation).

**SMD agreement**: quantification of the agreement between the emulated RWE and RCT treatment effect estimate. The SMD is calculated as

$$
SMD = \frac{\hat{\theta}_{RCT} - \hat{\theta}_{RWE}}{\sqrt{\text{Var}(\hat{\theta}_{RCT}) + \text{Var}(\hat{\theta}_{RWE})}}
$$

where $\hat{\theta}_{RCT}$ and $\hat{\theta}_{RWE}$ are the treatment effect estimates (hazard ratios or median survival times) and $\text{Var}(\hat{\theta}_{RCT})$ and $\text{Var}(\hat{\theta}_{RWE})$ are the corresponding variances for RCT and RWE, respectively. The resulting SMDs will be interpreted such that with an SMD of 1.00, the effect estimate from the RCT and the RWE emulation are 1 standard deviation apart. For an $\alpha$-level of 0.05, the null hypothesis of no difference would be rejected whenever $|Z| > 1.96$.

For the secondary endpoints of interest (e.g., median survival time or survival probabilities) only the SMD agreement metric will be applicable.

## Study design and statistical analysis

The study design for each trial emulation will be visualized as part of the protocol using a graphical depiction of the exact measurement windows of eligibility criteria, washout periods and covariates relative to the cohort entry time.[@schneeweiss2019graphical]

### Missing data

To establish an analytic cohort, key eligibility criteria will be applied in which patients with missing values in eligibility criteria are considered eligible in the respective attrition steps to allow for thorough missing data investigations. These missing data investigations will empirically assess assumptions on potentially underlying missingness mechanisms according to Rubin’s classification of missing data (i.e., missing completely at random \[MCAR\], missing at random \[MAR\] and missing not at random \[MNAR\]).[@rubin1976inference] To that end, we will adopt a principled process on missing data that was developed as part of a FDA Sentinel Innovation Center causal inference workstream that empirically evaluates different aspects across partially observed covariates based on three group diagnostics.[@weberpals2024smdi; @weberpals2024] In brief, these diagnostic cover (1) comparisons of patients characteristics with and without an observed level of the partially observed covariate, (2) ability to predict missingness given observed data, and (3) assessments if outcomes between patients with a missing value are systematically different. Together with expert domain knowledge and assumptions about the underlying missing data structure through canonical causal diagrams,[@moreno2018canonical] this will inform decisions regarding the in- or exclusion of patients with missing values in key eligibility criteria and potential sensitivity analyses to assess the robustness of these decisions.[@tompsett2018use]

While the MAR assumption is a strong assumption to hold across all considered covariates, it was shown that especially in the context of partially observed covariate data (as opposed to missing exposure and outcome data), only mechanisms in which a covariate causes its own missingness leads to critical bias (MNAR).[@moreno2018canonical] Hence, methodologies which retain patients and give the potential to adjust for a broader set of prognostic factors (e.g., multiple imputation[@weberpals2024hdmi] or doubly robust methods[@Shaw2024]) may be preferred over complete case analyses.

### Endpoints and propensity score analyses

Due to its ubiquity in oncology trials, the primary parameter of interest in ENCORE will be defined as the marginal hazard ratio (HR) coefficient for the treatment comparison for time to all-cause mortality (OS).[@cox1972regression] However, the HR has many limitations that can make the comparison of results across varying follow-up times between RCT and emulations challenging, including its non-collapsibility, in-built selection bias and the requirement of proportional hazards.[@hernan2010hazards; @stensrud2020test] For this reason, we will also consider alternative endpoints on an absolute risk scale such as median survival times, survival probabilities at pre-defined time points during follow-up[@kaplan1958nonparametric] and restricted mean survival times as secondary endpoints of interest.

For the estimation of marginal treatment effects, we will employ propensity score methods to adjust for measured confounding between treatment arms. The selection of important prognostic covariates will be based on expert clinical knowledge and published literature on prognostic scores in oncology.[@becker2020enhanced] The implementation of propensity scores in combination with multiple imputation will follow the '*within*' methodology as described by Leyrat et al.[@leyrat2019propensity; @pishgar2020matchthem] That is, propensity score matching or weighting will be applied to each imputed dataset. The marginal treatment effect will then be estimated in each imputed and matched or weighted dataset separately and pooled into a final estimate following Rubin's rule.[@rubin2018multiple; @van2011mice] This approach has been shown to lead to unbiased estimates across different simulated scenarios with a sufficient estimation of the variance.[@leyrat2019propensity]

To asses the balance of pre-exposure covariates after matching or weighting on each imputed dataset, the average SMD and corresponding minimum and maximum SMD range will be visualized (see example in @fig-balance). Covariate balance is typically considered at a SMD \< 0.1.[@austin2009balance] Further, we will compute the average post-matching or post-weighting C-statistics.[@franklin2014metrics] In addition, we will use a published prognostic score for OS[@becker2020enhanced] as a balance measure to visually assess if the prognostic score is balanced between treatment arms after propensity score matching or weighting as this approach was described to show the highest correlations with bias compared with other balance measures and not affected by model misspecification.[@stuart2013prognostic]

Similarly, survival probabilities for individual time points will be estimated in each imputed and propensity score matched or weighted dataset according to the Kaplan-Meier method.[@kaplan1958nonparametric] Since survival probabilities typically do not follow normal distributions which are required to apply Rubin's rule, these will be transformed through a complementary log-log transformation $log(-log(1-pr(surv)))$ with $pr(surv)$ denoting the survival probability at a given time during follow-up.[@warwick28685; @morisot2015prostate] The transformed survival probabilities are then pooled across imputed datasets and individual time points following Rubin's rule and back-transformed via $1-exp(-exp(qbar))$ with $qbar$ denoting the pooled survival probability. The median survival time can be finally determined by extracting the time point during follow-up at which the survival probability drops below 0.5 for the first time.

### Sensitivity analyses

Given that a large number of factors could influence a difference between RCT results and emulated trial results, a broad range of sensitivity analyses will be conducted. This can comprise decisions on the considered databases, calendar time period, covariate measurement (e.g., measurement windows or trade-offs on sensitivity versus specificity in measurements), approaches to missing data, selection of covariates for imputation and propensity score models and decisions on when to censor patients. All sensitivity analyses will be pre-specified in the study protocol and reported using appropriate visualizations such as forest plots.

### Reproducibility and consistency across emulation

To ensure a transparent, reproducible and consistent way of deriving analytic cohorts and performing statistical analyses across trial emulations, we developed an internal R package `encore.io` with parameterized functions. Detailed documentation can be found in the Supplementary Material.

{{< pagebreak >}}

# Discussion

Building on an established approach based on the RCT DUPLICATE project[@wang2023emulation; @franklin2020nonrandomized], the ENCORE project aims to emulate 12 oncology trials using multiple EHR data sources to inform the potential use of RWE for regulatory science in oncology. The project focuses on four common cancers and will evaluate the agreement of treatment effect estimates between RCTs and their respective emulations. Historically, administrative health claims databases have been the backbone of most research in RWE. With increasing access to EHR and a maturing set of methodological approaches to draw causal inferences from such data, the ENCORE project is one of the first of its kind to evaluate when and how RWD can be used to deliver similar causal conclusions compared to RCTs in the field of oncology.

The RCT DUPLICATE project[@wang2023emulation; @franklin2020nonrandomized] identified multiple emulation challenges which we expect to also encounter in ENCORE. One particular aspect that we may not always be able emulate is the exact distribution of patient characteristics of the trial population. This can be due to the lack of data granularity and comprehensiveness to emulate relevant eligibility criteria or the fact that large pivotal trials in oncology are typically conducted in multiple countries worldwide, which we will not be able to mirror given that all considered databases reflect the US only. This may be a critical factor especially given that the pathophysiology, prognosis and factors that drive heterogeneous treatment outcomes of certain cancers differ between countries. For example, some cancer indications (e.g., GI cancers) or some genetic mutations (e.g., EGFR mutations) are much more prevalent in Asian countries compared to the US or Europe.

Another common challenge in the emulation of oncology trials is the estimation of an "intention-to-treat" (ITT) analogous estimand which is usually the primary estimand reported in oncology RCTs. Due to intercurrent events, such as non-adherence, crossover of a high proportion of patients from the control to the intervention arm, or differences in subsequent therapy lines there may be estimand differences between trial and emulation.[@manitz2022estimands] Although this is also a common challenge in the analysis of RCTs,[@rufibach2018] treatment protocols in routine care are often observed to be less stringent compared to trials.[@merola2024calibrating] In order to derive comparable estimands it is therefore crucial to understand and contextualize the proportion and timing of treatment switching and discontinuation in both the trial and its emulation.

This issue may be even augmented with more complex treatment protocols like combination regimens (e.g., CheckMate9LA) as compared to monotherapies (e.g., CheckMate057) since the ascertainment of the exposure needs to happen over a pre-defined time window which is often difficult to calibrate in RWD. As a result, exposure misclassification (due to too narrow ascertainment windows) and selection bias (due to too long ascertainment windows) are common trade-offs in such scenarios. Alternative analytic approaches which target a per-protocol estimand, such as the clone-censor-weight design,[@gaber2024mystifying] may be viable options if the ITT estimand cannot be estimated due to the aforementioned parameters, provided that necessary covariate measurements are available to account for the artificial censoring introduced with these methods.

## Conclusions

RWE based on fit-for-purpose data and principled, well-designed and reproducible studies may complement evidence coming from RCTs. Through a systematic benchmarking approach, the ENCORE project will provide insights as to how measurement, design and analytic decisions influence bias and validity in oncological RWE studies.

{{< pagebreak >}}

# References {.unnumbered}

::: {#refs}
:::

{{< pagebreak >}}

# Tables {.unnumbered}

```{r}
#| label: tbl-criteria
#| tbl-cap: "Criteria to select eligible trials for emulation in ENCORE."
#| tbl-cap-location: top

# read excel blueprint
eligibility_criteria <- read_excel(
  path = here("tables", "helper_tables.xlsx"),
  sheet = "ctgov_criteria_defintions"
  )

# convert to gt table
tbl_eligibility_criteria <- eligibility_criteria |> 
  select(-Link) |> 
  filter(Criteria != "Study phase") |> 
  gt() |> 
  tab_style(
    style = cell_text(weight = "bold"),
      locations = cells_column_labels()
    )

# save table as .docx document
tbl_eligibility_criteria |> 
   gtsave(
    filename = here("manuscript", "Table_1_trial_eligibility.docx")
    )

# save image to display within manuscript document
suppressMessages(tbl_eligibility_criteria |> 
  gtsave(
    filename = here("tables", "Table_1_trial_eligibility.png"),
    expand = 10, # amount of whitespace,
    zoom = 5 # higher resolution
    ))

include_graphics(path = here("tables", "Table_1_trial_eligibility.png"))
```

{{< pagebreak >}}

```{r}
#| label: tbl-rcts
#| tbl-cap: "Tentative list of randomized controlled trials (RCTs) considered for emulation."
#| tbl-cap-location: top

rct_selection <- read_excel(
  path = here("tables", "helper_tables.xlsx"),
  sheet = "rct_selection"
  )

tbl_rct_selection <- rct_selection |> 
  group_by(cancer) |> 
  gt() |> 
  cols_label(
    nctid = md("**NCTID**"),
    acronym = md("**Acronym**"),
    setting = md("**Clinical setting**"),
    line = md("**Line of therapy**"),
    exposures = md("**Treatment comparison**")
    ) |> 
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_row_groups()
    )

# save table as .docx document
tbl_rct_selection |> 
   gtsave(
    filename = here("manuscript", "Table_2_trial_selection.docx")
    )

# save image to display within manuscript document
suppressMessages(tbl_rct_selection |> 
  gtsave(
    filename = here("tables", "Table_2_trial_selection.png"),
    expand = 10, # amount of whitespace,
    zoom = 5 # higher resolution
    ))

include_graphics(path = here("tables", "Table_2_trial_selection.png"))
```

{{< pagebreak >}}

```{r}
#| label: tbl-metrics
#| tbl-cap: "Visualization of agreement metrics example."

source(here("functions","smd_agreement.R"))

data <- tribble(
  ~trial, ~group, ~estimate, ~lower, ~upper,
  "Trial 1", "RCT", 0.75, 0.61, 0.91,
  "Trial 1", "RWE", 0.80, 0.51, 1.1,
  "Trial 2", "RCT", 0.62, 0.51, 0.71,
  "Trial 2", "RWE", 0.65, 0.55, 0.69,
  "Trial 3", "RCT", 0.71, 0.67, 0.80,
  "Trial 3", "RWE", 0.51, 0.41, 0.61,
  "Trial 4", "RCT", 0.90, 0.81, 0.99,
  "Trial 4", "RWE", 1.2, 1.09, 1.34,
  ) |> 
  pivot_wider(names_from = group, values_from = c(estimate, lower, upper)) |> 
  mutate(
    smd_value = smd_agreement(
      rct_estimate = log(estimate_RCT), 
      rct_lower = log(lower_RCT), 
      rct_upper = log(upper_RCT),
      rwe_estimate = log(estimate_RWE),
      rwe_lower = log(lower_RWE), 
      rwe_upper = log(upper_RWE)
      )
    ) |> 
  mutate(
    regulatory = ifelse(
      # superiority
      estimate_RCT < 1 & upper_RCT < 1 & estimate_RWE < 1 & upper_RWE < 1 |
        # non-inferiority
        estimate_RCT < 1 & upper_RCT >= 1 & estimate_RWE < 1 & upper_RWE >= 1 |
        # inferiority (made up)
        estimate_RCT >= 1 & lower_RCT >= 1 & estimate_RWE >= 1 & upper_RWE >= 1,
      "Yes", "No"
      ),
    estimate = ifelse(
      estimate_RWE >= lower_RCT & estimate_RWE <= upper_RCT, "Yes", "No"
      ),
    smd = ifelse(
      abs(smd_value) < 1.96, "Yes", "No"
      )
    )

tbl_metrics <- data |> 
  mutate(across(where(is.numeric), function(x) format(x, digits = 2, nsmall = 2))) |> 
  mutate(
    RCT = glue("{estimate_RCT} ({lower_RCT} - {upper_RCT})"),
    RWE = glue("{estimate_RWE} ({lower_RWE} - {upper_RWE})"),
    smd = glue("{smd} ({smd_value})")
  ) |> 
  select(trial, RCT, RWE, regulatory, estimate, smd) |> 
  gt() |> 
  tab_spanner(
    label = "HR (95% CI)",
    columns = c(RCT, RWE)
  ) |> 
  cols_label(
    trial = "Trial",
    regulatory = md("Statistical <br> significance <br> agreement"),
    estimate = md("Estimate <br> agreement"),
    smd = "SMD"
    ) |> 
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
    ) |> 
    tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_spanners()
    ) |> 
  tab_style(
    style = cell_text(color = "darkgreen"), 
    locations = cells_body(
      columns = regulatory,
      rows = regulatory == "Yes"
      )
    ) |> 
  tab_style(
    style = cell_text(color = "darkred"), 
    locations = cells_body(
      columns = regulatory,
      rows = regulatory == "No"
      )
    ) |> 
  tab_style(
    style = cell_text(color = "darkgreen"), 
    locations = cells_body(
      columns = estimate,
      rows = estimate == "Yes"
      )
    ) |> 
  tab_style(
    style = cell_text(color = "darkred"), 
    locations = cells_body(
      columns = estimate,
      rows = estimate == "No"
      )
    ) |> 
  tab_style(
    style = cell_text(color = "darkgreen"), 
    locations = cells_body(
      columns = smd,
      rows = stringr::str_detect(smd, "Yes")
      )
    ) |> 
  tab_style(
    style = cell_text(color = "darkred"), 
    locations = cells_body(
      columns = smd,
      rows = stringr::str_detect(smd, "No")
      )
    ) |> 
  tab_footnote("Abbreviations: CI = Confidence interval, HR = Hazard ratio, RCT = Randomized controlled trial, RWE = Real-world evidence, SMD = standardized mean difference (based on log hazard ratios)")

# save table as .docx document
tbl_metrics |> 
   gtsave(
    filename = here("manuscript", "Table_3_agreement_metrics.docx")
    )

# save image to display within manuscript document
suppressMessages(tbl_metrics |> 
  gtsave(
    filename = here("tables", "Table_3_agreement_metrics.png"),
    expand = 10, # amount of whitespace,
    zoom = 5 # higher resolution
    ))

include_graphics(path = here("tables", "Table_3_agreement_metrics.png"))
```

{{< pagebreak >}}

# Figures {.unnumbered}

```{r}
#| label: fig-process
#| fig-cap: "Systematic process to understand effectiveness claims of oncology trials using real-world evidence."
#| fig-cap-location: top
#| fig-width: 12

knitr::include_graphics(here("figures", "process.png"))

# copy figure to manuscript
file_copy(
  path = here("figures", "process.png"),
  new_path = here("manuscript", "Figure_1_process.png"),
  overwrite = TRUE
  )
```

[View figure in higher resolution here](https://github.com/janickweberpals/encore-process-manuscript/blob/main/manuscript/Figure_1_process.png)

{{< pagebreak >}}

```{r}
#| label: fig-initiators
#| fig-cap: "Visualization of descriptive drug utilization analyses displaying a) initiation trends between compared regimens based on calendar time, b) cumulative rate of patients switching to another line of treatment."
#| fig-widh: 3
#| fig-height: 6

data <- tibble::tibble(
  drug = c(rep("Drug A", 5), rep("Drug B", 5)),
  initiators = c(0, 220, 300, 399, 500, 456, 400, 278, 180, 35),
  year = rep(c(2011, 2012, 2013, 2014, 2015), 2)
  ) 

initiators <- data |> 
  ggplot(aes(x = year, y = initiators, group = drug, color = drug)) +
  geom_line(linewidth = 1) +
  labs(
    x = "Calendar Year",
    y = "# initiators",
    color = "Exposure"
    ) +
  scale_color_manual(values=c('orange','blue')) +
  theme_minimal(base_size = 10)

# treatment switch
data <- tibble::tibble(
  drug = c(rep("Drug A", 6), rep("Drug B", 6)),
  prop = c(4.1, 7.7, 17.7, 22.2, 24.4, 24.5, 4.6, 13.2, 25.1, 29, 29.3, 31.3),
  time = rep(seq(1, 12, 2), 2)
  ) 

switchers <- data |> 
  ggplot(aes(x = time, y = prop, group = drug, color = drug)) +
  geom_line(linewidth = 1) +
  labs(
    x = "Follow-up time [Months]",
    y = "Cumulative switch rate",
    color = "Exposure"
    ) +
  scale_color_manual(values=c('orange','blue')) +
  theme_minimal(base_size = 10)

# combine plots with shared legend
suppressWarnings(shared_legend <- get_legend(initiators))

plot_utilization <- plot_grid(
  plot_grid(initiators + theme(legend.position = "top")),
  plot_grid(switchers + theme(legend.position = "none")),
  ncol = 1,
  nrow = 2,
  labels = c("a)", "b)"),
  shared_legend
  )

# save
suppressMessages(ggsave(
  plot = plot_utilization,
  filename = "Figure_2_utlization.png",
  path = here("manuscript"),
  dpi = 800
  ))

# print to manuscript
include_graphics(path = here("manuscript", "Figure_2_utlization.png"))
```

[View figure in higher resolution here](https://github.com/janickweberpals/encore-process-manuscript/blob/main/manuscript/Figure_2_utlization.png)

{{< pagebreak >}}

```{r}
#| label: fig-balance
#| fig-cap: "Assessment of a) covariate balance and b) distributional balance of a prognostic score for overall survival before and after propensity score matching or weighting across multiple imputed datasets."
#| fig-widh: 4
#| fig-height: 7

source(here("functions", "simulate_data.R"))

data <- simulate_data(
  n_total = 2000, 
  seed = 41, 
  include_id = FALSE, 
  imposeNA = TRUE,
  propNA = .25
  ) |> 
  mutate(treat = ifelse(treat == 1, "Drug A", "Drug B"))
 
# impute
mids_data <- suppressWarnings(mice(data, m = 10, printFlag = F))

# weight
covariates <- data |> 
  select(starts_with("dem_"), starts_with("c_"))

# ps formula
ps_formula <- as.formula(paste0("treat ~ ", paste0(colnames(covariates), collapse = " + ")))

# weighting  
suppressMessages(wimids_data <- weightthem(
  datasets = mids_data, 
  formula = ps_formula, 
  method = "glm",
  estimand = "ATO"
  ))

# Create the named character vector
# named_covariates <- setNames(paste("Covariate", seq_len(ncol(covariates))), colnames(covariates))

# covariate balance
covariate_balance <- love.plot(
  x = wimids_data,
  abs = TRUE,
  thresholds = 0.1, 
  drop.distance = TRUE,
  var.order = "unadjusted",
  colors = c("orange", "blue"), 
  stars = "std",
  shapes = 17, 
  size = 4, 
  grid = TRUE,
  position = "top"
  )

# distributional balance
# Simulate data
set.seed(42)  # For reproducibility
n <- 2000  # Number of samples per group
drugA_pre <- rnorm(n, mean = 0, sd = 2.5 / 2)  # Spread = 2.5
drugB_pre <- rnorm(n, mean = -1, sd = 2.5 / 2)  # Spread = 2.5
drugA_post <- rnorm(n, mean = 0, sd = 2.5 / 2)  # Spread = 2.5
drugB_post <- rnorm(n, mean = 0, sd = 2.5 / 2)  # Spread = 2.5

# Combine into a data frame
data <- data.frame(
  Value = c(drugA_pre, drugA_post, drugB_pre, drugB_post),
  Group = rep(c("Drug A", "Drug A", "Drug B", "Drug B"), each = n),
  PrePost = rep(c("Unadjusted sample", "Adjusted sample", "Unadjusted sample", "Adjusted sample"), each = n)
  ) |> 
  mutate(PrePost = factor(PrePost, levels = c("Unadjusted sample", "Adjusted sample")))

# Create the plot
score_balance <- ggplot(data, aes(x = Value, fill = Group)) +
  geom_density(alpha = 0.5) +
  scale_fill_manual(values = c("orange", "blue")) +
  labs(
    title = "Distributional balance for prognostic score",
    x = "Value",
    y = "Density",
    fill = "Group"
  ) +
  theme_minimal() +
  theme(legend.position = "top") +
  facet_wrap(~PrePost)

plot_balance <- suppressMessages(plot_grid(
  plot_grid(covariate_balance),
  plot_grid(score_balance),
  ncol = 2,
  nrow = 1,
  labels = c("a)", "b)"),
  shared_legend
  ))

# save
suppressMessages(ggsave(
  plot = plot_balance,
  filename = "Figure_3_balance.png",
  path = here("manuscript"),
  dpi = 800,
  width = 15,
  height = 8
  ))

# print to manuscript
include_graphics(path = here("manuscript", "Figure_3_balance.png"))
```

[View figure in higher resolution here](https://github.com/janickweberpals/encore-process-manuscript/blob/main/manuscript/Figure_3_balance.png)
