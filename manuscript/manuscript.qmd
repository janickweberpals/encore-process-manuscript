---
title: "A Process for the Emulation of Comparative Oncology Trials with Real-world Evidence (ENCORE)"

code-fold: false
echo: false
number-sections: false
warnings: false
messages: false

format: 
  docx:
    fig-cap-location: top
    tbl-cap-location: top
    reference-doc: custom-reference-doc.docx
  pdf:
    include-in-header: 
      text: |
        \usepackage{lscape}
        \newcommand{\blandscape}{\begin{landscape}}
        \newcommand{\elandscape}{\end{landscape}}
    fig-cap-location: top
    tbl-cap-location: top
    
editor: visual
bibliography: references.bib
csl: journal-of-clinical-oncology.csl

filters:
  - docx-landscape.lua
---

```{r}
#| label: setup
#| include: false

library(here)
library(dplyr)
library(knitr)
library(gt)
library(readxl)
library(fs)
library(webshot2)
library(tibble)
library(ggplot2)
library(cowplot)
library(mice)
library(MatchThem)
library(cobalt)
```

**Authors**: Janick Weberpals^1^, Kenneth L. Kehl^2^, Donna R. Rivera^3^, Pallavi Mishra-Kalyani^3^, ..., Sebastian Schneeweiss^1^, Shirley V. Wang^1^

[Author affiliations:]{.underline}

^1^ Division of Pharmacoepidemiology and Pharmacoeconomics, Department of Medicine, Brigham and Women's Hospital, Harvard Medical School, Boston, MA, USA

^2^ Dana-Farber Cancer Institute, Boston, MA, USA

^3^ Oncology Center of Excellence, US Food and Drug Administration, Silver Spring, MD, USA

[**Correspondence:**]{.underline}

Shirley V. Wang, PhD

Division of Pharmacoepidemiology and Pharmacoeconomics,

Department of Medicine, Brigham and Women's Hospital, Harvard Medical School,

1620 Tremont Street, Suite 3030-R, Boston, MA 02120, USA

Phone: +1 617-278-0932

Fax: + 1 617-232-8602

Email: [SWANG1\@BWH.HARVARD.EDU](SWANG1@BWH.HARVARD.EDU)

[**Article type:**]{.underline} Review

[**Word count:**]{.underline} xxx words / xxx words

[**Tables:**]{.underline} x

[**Figures:**]{.underline} x

[**Supplementary material:**]{.underline} Supplementary tables and figures

[**Short running title**]{.underline}: ...

[**Keywords:**]{.underline} Oncology, Real-World Evidence, Trial emulation, EHR

[**Funding Statement:**]{.underline} This project was supported by ...

[**Competing Interests Statement:**]{.underline} Dr. Weberpals is now an employee of AstraZeneca and owns stocks in AstraZeneca. Dr. Wang has consulted ad hoc for Exponent Inc. and MITRE a federally funded research center for the Centers for Medicare and Medicaid Services on unrelated work. Dr. Schneeweiss is participating in investigator-initiated grants to the Brigham and Women's Hospital from Boehringer Ingelheim, Takeda, and UCB unrelated to the topic of this study. He owns equity in Aetion Inc., a software manufacturer. He is an advisor to Temedica GmbH, a patient-oriented data generation company. His interests were declared, reviewed, and approved by the Brigham and Women's Hospital in accordance with their institutional compliance policies.

[**Data sharing statement:**]{.underline} ...

[**Analytic code sharing statement:**]{.underline} ...

[***Proposed target journals:***]{.underline} *CPT =\> JCO CCI =\> ...*

*Manuscript last updated: `r Sys.time()`*

{{< pagebreak >}}

# Abstract {.unnumbered}

xxx words/xxx words

...

{{< pagebreak >}}

# Background

Randomized controlled trials (RCTs) have been the gold standard for establishing the efficacy and safety of medical products. With the advent of the the 21^st^ Century Cures Act directive[@RWEFDA], the Food and Drug Administration (FDA) established a framework to increasingly consider real-world evidence (RWE) generated from routine-care health data such as electronic health records (EHR) to evaluate and contextualize the comparative safety and effectiveness of novel cancer therapies.[@purpura2022role] With 21% of all approvals, oncology was the disease area with the most FDA drug approvals in 2023,[@senior2024fresh] and especially in the field of precision oncology, RWE has a large potential to complement evidence coming from RCTs. Potential use cases comprise the assessment of effectiveness in patient populations that are underrepresented in RCTs, the construction of external control arms in single-arm trials where active recruitment may not be feasible or the discovery of biomarkers among pan-tumor populations that harbor specific genomic and immuno-pathological signatures.

However, the validity and transportability of results derived betwen RWE studies and RCTs can depend on many factors and frequently referenced limitations include missing data, small sample sizes, data discontinuity [@Merola2022; @joshua2022longitudinal], rapid changes in guideline treatment patterns and the inability to measure and emulate common eligibility criteria and prognostic factors in real-world data (RWD).[@rider2024emulations] While there are already published examples of oncology trial emulations[@rider2024emulations; @merola2023aetion; @merola2024calibrating], a systematic and scaled approach to emulate a diverse set of different oncology trials in multiple heterogeneous databases is necessary to gain confidence in the accuracy of RWE studies and to provide an answer as to which questions can be validly answered.

The RCT DUPLICATE initiative[@wang2023emulation] increased our understanding of when RWE studies can come to causal conclusions on treatment effects by comparing results against RCTs under the assumption that each RCT finding reflects a causal treatment effect. In settings where the RCT designs could be emulated well, RWE studies came to the same conclusions.[@heyard2024design] However, prior work from RCT-DUPLICATE has focused primarily on emulating trials in the cardio-metabolic, renal, and pulmonary clinical areas using claims databases.

The *Emulation of Comparative Oncology Trials with Real-world Evidence* (**ENCORE**) project [@encoreFDA] aims to extend this work to the field of oncology which comes with its own unique set of challenges which must be systematically explored and understood. Building on a process co-developed with the FDA through RCT DUPLICATE[@wang2023emulation], this expansion to oncology is going to emulate 12 randomized clinical trials using multiple EHR data sources. The process includes an emphasis on transparency with documented assessment of data fitness of the RWD source for each trial[@rivera2024oncology; @gatto2022structured] and the conduct of extensive sensitivity analyses to assess robustness of findings and trial eligibility criteria.

The objectives of this project are to develop state-of-the-art methodological approaches and apply these to create insights that may provide guidance on the potential use of RWE for regulatory science in oncology. This includes the systematic evaluation of the suitability of data in relation to the study design and statistical analysis by emulating 12 oncology trials across four cancers and assessing the agreement of treatment effect estimates between RCTs and their respective emulations.

In this process paper, we describe the design and process for the selection of the 12 oncology RCTs, the assessment of the database quality and selection, protocol development, study design and statistical analysis and final agreement metrics to evaluate the concordance between RCTs and emulations.

# Methods

A visual summary of the entire systematic process from trial selection to final results is provided in @fig-process.

## Trial selection

The focus of ENCORE is to maximize potential learnings on when RWE studies can or cannot yield similar results compared to RCTs. To that end, the emphasis of the project is on trials of therapies for the most common cancers and/or cancers for which there has been substantial therapeutic development in recent years. After careful review and exchange with clinical and regulatory experts, four cancer indications were identified including lung cancer, breast cancer, colorectal cancer and multiple myeloma. For each cancer we aim to conduct three trial emulations which will be implemented using multiple databases accessible for the scope of this project (i.e. the total number of emulations will equal 12 trials x *n* databases which are found fit-for-purpose for each trial).

The trial selection will follow a semi-automated process for which we will document the eligibility criteria resulting in a CONSORT diagram showing reasons for excluding RCTs. The search will be conducted using the AACT database which is s a publicly available relational database developed and maintained by the Clinical Trials Transformation Initiative (CTTI) which contains all information (protocol and result data elements) about every study registered on ClinicalTrials.gov.[@tasneem2012database] To identify eligible trials, we will use a combined search query strategy of the National Library of Medicine (NLM)-controlled *MeSH* term and a free keyword search for the respective cancer indication in the *conditions*, *studies* and *detailed_descriptions* fields of each trial entry on ClinicalTrials.gov.

Eligible trials need to fulfill the following basic criteria:

-   Interventional

-   Randomized

-   Intervention model: parallel assignment

-   Industry-sponsored

-   Trial start in 2011 or later

-   Primary purpose was to study treatment effects

-   Overall survival must be one of the endpoints reported (either as hazard ratio or median overall survival time)

-   Recruitment status: ‘Completed’ or ‘Active, not recruiting’

-   Feasibility and clinical relevance

The rationale and operationalization of each criterion is listed in detail in @tbl-criteria. We will mainly consider pivotal interventional, randomized trials after 2011 since treatment guidelines among included cancer indications have undergone significant changes in recent years. Due to the rapid adoption of new breakthrough therapies in routine care, it is unlikely to find patients who may be still treated with outdated treatment regimens in the real-world. In parallel, trial results should have also not been published too recently in order to allow for enough data and follow-up time accrual in databases used for this project. A further focus is on trials that have reported overall survival (OS) as one of the pre-specified endpoints in the protocol. Although there have been substantial methodological advancements to increase our understanding on the emulation and comparison of real-world progression-free survival (PFS) and objective response rates (ORR) to a RECISTv1.1[@eisenhauer2009new]-based PFS and ORR assessment in RCTs [@ThanCCR; @mckelvey2024evaluation], imaging-based evaluations still hold a level of granularity which may not be necessarily reflected in chart-abstracted assessments of a patient's progression in routine care.[@rwdRECIST; @rivera2022friends] Given the large number of other methodological challenges like missing data, small sample sizes, data discontinuity and rapidly changing guideline treatments, the scope of this project was to focus on the emulation of OS as the endpoint of interest.

While most trial-eligibility criteria can be operationalized in an automated fashion, the last criterion on emulation feasibility and clinical relevance involves extensive human review. The critical points considered in this step include a thorough feasibility assessment of the data fitness [@rivera2022friends], sample size considerations and the assessment if critical eligibility criteria (e.g., biomarker status) and prognostic factors (e.g., ECOG performance score) are measurable and can be balanced using propensity score matching or weighting methods.[@brookhart2006variable] Lastly, trial candidates are ranked and shortlisted into primary and runner-up candidates based on their clinical and regulatory relevance.

A list of tentative, shortlisted primary candidates is presented in @tbl-rcts and the corresponding selection process is illustrated in the CONSORT diagrams (Supplementary Figures 1-4). Naturally, the majority of trials will cover advanced or metastatic cancer populations since a large proportion of drug development efforts have focused on these settings in recent years. A key learning that we aim to foster with the shortlisted trials is to achieve a better understanding how different disease settings (early, late), line settings (\[neo\]adjuvant, first line, advanced lines of therapy), therapy protocols (monotherapy, combination therapy) and population characteristics (simple versus complex genetic or immunological signatures) can be emulated using RWD. If ongoing feasibility assessments indicate that these trials cannot be emulated with high enough confidence, runner-up candidates will be considered instead.

## Databases

The ENCORE project will utilize data from a total of four different oncology-specific electronic health records (EHR)-derived specialty oncology data sources: ConcertAI, COTA, Flatiron Health, McKesson/Ontada. All available databases draw from a comprehensive national sample of patients with cancer in the US with detailed EHR-derived information on the information necessary to study medication effectiveness in oncology. A detailed description and methodology on how patients are sampled will be provided for each emulation protocol individually. For ENCORE, not all databases will be available for each cancer indication and the names of the databases will be blinded and referred to as ENCORE DataBase (EDB) 1, 2, 3 and 4 for the final reporting of results. If more than one database is considered fit-for-purpose for a respective trial emulation, the best possible analytic model will be employed for each database separately and final treatment effect estimates will be pooled using a meta-analytic approach.

<!--# maybe data partners have  1-2 sentences and/or publications to reference? -->

## Protocol development

For each shortlisted and selected RCT, a detailed protocol, pre-specifying key elements of the trial emulation, will be developed following the HARPER protocol template[@wang2022harmonized] and will be registered on ClinicalTrials.gov after careful review by a clinical and FDA regulatory expert panel. Following the target trial emulation framework, we will provide an explicit statement and rationale on how each element will be emulated including database selection, covariate measurement, operationalization of key eligibility criteria, study design, data analysis and causal contrasts of interest.[@hernan2022target; @hernan2016specifying] Since it is common that oncology RCTs update OS estimates periodically based on accrued follow-up time, the protocol will give a brief summary of each emulated RCT and specify which target OS estimates will be used to compare agreement metrics to (see @sec-agreement-metrics). All eligibility criteria will be extracted based on publicly available protocols and statistical analysis plans of the selected RCT.

### Emulation feasibility

**Fit-for-purpose data.** Real-world data fitness and emulation feasibility for a given shortlisted candidate trial will be assessed in multiple steps based on guidance of the oncology quality, characterization, and assessment of real-world data (Oncology QCARD) Initiative.[@rivera2022friends] The first step assesses if relevant variables like exposure/line of therapy, outcomes, and covariates are generally available, measured and operationalizable in routine-care. Since a vast majority of oncological RCTs in recent years have focused on selected, biomarker-defined populations, subtleties in measurement and operationalizability of specific biomarkers must be reflected to ensure a representative and large enough study population. For example, immunotherapies have significantly changed the cancer treatment landscape since the approval of the first PD-L1 inhibitor in 2015. With many trials that have followed thereafter, the operationalization of the expression of the PD-L1 biomarker in RCTs (e.g., as a percent staining, tumor proportion score or combined positive score) has also evolved since then and PD-L1 '*positivity'* may have different definitions across calendar years based on different cut-off values.

According to the *Structured Process to Identify Fit-For-Purpose Data* (SPIFD) framework[@gatto2022structured], the next step will outline tables that describe how eligibility criteria will be ascertained using a color-coded heatmap that will indicate the level of confidence on how well each criterion can be emulated in each selected database. As there are general eligibility criteria in oncology trials which either won't be possible to emulate (e.g., physician-assessed survival prognosis of xy months) or that are clinically not relevant for the emulation of the trial (e.g., male patients should be willing to use barrier contraception), the study team will decide on key eligibility criteria for the emulation of the trial.

We will additionally provide a definition on how exposures, outcomes and covariates are exactly defined and operationalized in each respective database. There will be special emphasis on how exposure, in context of their respective disease and line of therapy settings, and the OS outcome will be emulated. For all considered databases, the OS endpoint is typically a composite that is derived from different sources comprising EHR abstractions, social security death index, obituary and other linkages. Given that not all relevant sources that provide mortality data are synchronized and updated uniformly, sensitivity analyses with more conservative (i.e., earlier) censoring dates will be considered for each trial emulation to mitigate the potential impact of ghost-time bias.[@meyer2020open]

**Descriptives and data exploration.** Critical aspects when emulating oncology trials are the choice and estimation of the appropriate estimand of interest.[@rufibach2018] Particularly when emulating pivotal trials of paradigm-changing treatments, multiple aspects need to be considered such as the contemporaneity of the (historical) control cohort, the adoption rate of the novel intervention in routine care, the magnitude of the clinical treatment benefit and the rate in which (particularly patients in the control arm) discontinue or cross-over to the interventional treatment, which could lastly bias emulated treatment effects towards the null. To that end, comprehensive data explorations will be performed as part of the protocol development to contextualize these parameters and (if reported) draw comparisons to the emulated trial. Example for such standard diagnostics are visualized in @fig-initiators.

The distribution of patient characteristics, stratified by exposure status, will be examined in Table 1's before and after applying eligibility criteria and contrasted with the distributions of patient characteristics of the original RCT. Initial propensity score matching or weighting methods will be applied to ensure that measured pre-exposure covariates can be balanced, eposure cohorts are conditionally exchangeable at baseline and resulting sample sizes are still sufficient after matching or weighting. At this stage, all exploratory analyses will be conducted blinded towards the outcome to not bias any study design and analytic choices based on known outcome information.

**Statistical power**. Causal analyses of observational data may not have the same pre-requisites in terms of formal hypothesis testing and statistical power than RCTs since the number of 'recruited' patients is given and cannot be influenced.[@hernan2022causal] For this project, however, statistical power is a critical to assess the feasibility that a comparison of agreement metrics of estimated parameters between RCT and RWD with sufficient precision can be made. Since the main outcome of interest is defined as time to all-cause mortality (OS), the estimation of the statistical power is driven by the number of events rather than the number of patients. To assess if the unstratified number of events is sufficient such that a significant difference can be detected based on the original RCT-reported hazard ratio (HR), the statistical power $\beta$ will be estimated using Schoenfeld's sample-size formula for the proportional-hazards regression model.[@schoenfeld1981asymptotic; @schoenfeld1983sample; @gsDesign]

<!--# https://github.com/keaven/gsDesign/blob/master/R/nEvents.R#L22C12-L22C78 -->

## Study design and statistical analysis

The study design for each trial emulation will be visualized as part of the protocol using a graphical depiction of the exact measurement windows of eligibility criteria, washout periods and covariates relative to the cohort entry time.[@schneeweiss2019graphical]

### Missing data

To establish an analytic cohort, key eligibility criteria will be applied in which patients with missing values in eligibility criteria are considered eligible in the respective attrition steps to allow for thorough missing data investigations. These missing data investigations will empirically assess assumptions on potentially underlying missingness mechanisms according to Rubin’s classification of missing data (i.e., missing completely at random \[MCAR\], missing at random \[MAR\] and missing not at random \[MNAR\]).[@rubin1976inference] To that end, we will adopt a principled process on missing data that was developed as part of a FDA Sentinel Innovation Center causal inference workstream that empirically evaluates different aspects across partially observed covariates based on three group diagnostics.[@weberpals2024smdi; @weberpals2024] In brief, these diagnostic cover (1) comparisons of patients characteristics with and without an observed level of the partially observed covariate, (2) ability to predict missingness given observed data, and (3) assessments if outcomes between patients with a missing value are systematically different. Together with expert domain knowledge and assumptions about the underlying missing data structure through canonical causal diagrams[@moreno2018canonical], this will inform decisions regarding the in- or exclusion of patients with missing values in key eligibility criteria and potential sensitivity analyses to assess the robustness of these decisions.[@tompsett2018use]

While the MAR assumption is a strong assumption to hold across all considered covariates, it was shown that especially in the context of partially observed covariate data (as opposed to missing exposure and outcome data), only mechanisms in which a covariate causes its own missingness leads to critical bias (MNAR).[@moreno2018canonical] Hence, methodologies which retain patients and give the potential to adjust for a broader set of prognostic factors (e.g., multiple imputation[@weberpals2024hdmi] or doubly robust methods[@Shaw2024]) may be preferred over complete case analyses.

### Endpoints and propensity score analyses

Due to its ubiquity in oncology trials, the primary parameter of interest in ENCORE will be defined as the marginal hazard ratio (HR) coefficient for the treatment comparison for time to all-cause mortality (OS).[@cox1972regression] However, the HR has many limitations that can make the comparison of results across varying follow-up times between RCT and emulations challenging, including its non-collapsibility, in-built selection bias and the requirement of proportional hazards.[@hernan2010hazards; @stensrud2020test] For this reason, we will also consider alternative endpoints on an absolute risk scale such as median survival times, survival probabilities at pre-defined time points during follow-up[@kaplan1958nonparametric] and restricted mean survival times as secondary endpoints of interest.

For the estimation of marginal treatment effects, we will employ propensity score analytics to adjust for measured confounding between treatment arms. The selection of important prognostic covariates will be based on expert clinical knowledge and published literature on prognostic scores in oncology.[@becker2020enhanced] The implementation of propensity scores in combination with multiple imputation will follow the '*within*' methodology as described by Leyrat et al.[@leyrat2019propensity; @pishgar2020matchthem] That is, propensity score matching or weighting will be applied to each imputed dataset. The marginal treatment effect will then be estimated in each imputed and matched or weighted dataset separately and pooled into a final estimate following Rubin's rule.[@rubin2018multiple; @van2011mice] This approach has been shown to lead to unbiased estimates across different simulated scenarios with a sufficient estimation of the variance.[@leyrat2019propensity]

To asses the balance of pre-exposure covariates after matching or weighting on each imputed dataset, the average standardized mean difference (SMD) and corresponding minimum and maximum SMD range will be visualized (see example in @fig-balance). Covariate balance is typically considered at a SMD \< 0.1.[@austin2009balance] Further, we will compute the average post-matching or post-weighting C-statistics.[@franklin2014metrics] In addition, we will use a published prognostic score fro OS[@becker2020enhanced] as a balance measure to visually assess if the prognostic score is balanced between treatment arms after propensity score matching or weighting as this approach was described to show the highest correlations with bias compared with other balance measures and not affected by model misspecification.[@stuart2013prognostic]

Similarly, survival probabilities for individual time points will be estimated in each imputed and propensity score matched or weighted dataset according the Kaplan-Meier method.[@kaplan1958nonparametric] Since survival probabilities typically don't follow normal distributions which are required to apply Rubin's rule, these will be transformed through a complementary log-log transformation $log(-log(1-pr(surv)))$ with $pr(surv)$ denoting the survival probability at a given time during follow-up.[@warwick28685; @morisot2015prostate] The transformed survival probabilities are then pooled across imputed datasets and individual time points following Rubin's rule and back-transformed via $1-exp(-exp(qbar))$ with $qbar$ denoting the pooled survival probability. The median survival time can be finally determined by extracting the time point during follow-up at which the survival probability drops below 0.5 for the first time.

### Reproducibility and consistency across emulation

## Agreement metrics {#sec-agreement-metrics}

{{< pagebreak >}}

# Discussion

Target populations in international trials...genetic setup.

## Conclusions

...

{{< pagebreak >}}

# References {.unnumbered}

::: {#refs}
:::

{{< pagebreak >}}

# Tables {.unnumbered}

```{r}
#| label: tbl-criteria
#| tbl-cap: "Criteria to select eligible trials for emulation in ENCORE."
#| tbl-cap-location: top

# read excel blueprint
eligibility_criteria <- read_excel(
  path = here("tables", "helper_tables.xlsx"),
  sheet = "ctgov_criteria_defintions"
  )

# convert to gt table
tbl_eligibility_criteria <- eligibility_criteria |> 
  select(-Link) |> 
  filter(Criteria != "Study phase") |> 
  gt() |> 
  tab_style(
    style = cell_text(weight = "bold"),
      locations = cells_column_labels()
    )

# save table as .docx document
tbl_eligibility_criteria |> 
   gtsave(
    filename = here("manuscript", "Table_1_trial_eligibility.docx")
    )

# save image to display within manuscript document
suppressMessages(tbl_eligibility_criteria |> 
  gtsave(
    filename = here("tables", "Table_1_trial_eligibility.png"),
    expand = 10, # amount of whitespace,
    zoom = 5 # higher resolution
    ))

include_graphics(path = here("tables", "Table_1_trial_eligibility.png"))
```

{{< pagebreak >}}

```{r}
#| label: tbl-rcts
#| tbl-cap: "Tentative list of randomized controlled trials (RCTs) considered for emulation."
#| tbl-cap-location: top

rct_selection <- read_excel(
  path = here("tables", "helper_tables.xlsx"),
  sheet = "rct_selection"
  )

tbl_rct_selection <- rct_selection |> 
  group_by(cancer) |> 
  gt() |> 
  cols_label(
    nctid = md("**NCTID**"),
    acronym = md("**Acronym**"),
    setting = md("**Clinical setting**"),
    line = md("**Line of therapy**"),
    exposures = md("**Treatment comparison**")
    ) |> 
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_row_groups()
    )

# save table as .docx document
tbl_rct_selection |> 
   gtsave(
    filename = here("manuscript", "Table_2_trial_selection.docx")
    )

# save image to display within manuscript document
suppressMessages(tbl_rct_selection |> 
  gtsave(
    filename = here("tables", "Table_2_trial_selection.png"),
    expand = 10, # amount of whitespace,
    zoom = 5 # higher resolution
    ))

include_graphics(path = here("tables", "Table_2_trial_selection.png"))
```

{{< pagebreak >}}

{{< pagebreak >}}

# Figures {.unnumbered}

```{r}
#| label: fig-process
#| fig-cap: "Systematic process to understand effectiveness claims of oncology trials using real-world evidence."
#| fig-cap-location: top
#| fig-width: 12

knitr::include_graphics(here("figures", "process.png"))

# copy figure to manuscript
file_copy(
  path = here("figures", "process.png"),
  new_path = here("manuscript", "Figure_1_process.png"),
  overwrite = TRUE
  )
```

[View figure in higher resolution here](https://github.com/janickweberpals/encore-process-manuscript/blob/main/figures/process.png)

{{< pagebreak >}}

```{r}
#| label: fig-initiators
#| fig-cap: "Descriptive drug utilization analyses."
#| fig-widh: 4
#| fig-height: 7

data <- tibble::tibble(
  drug = c(rep("Drug A", 5), rep("Drug B", 5)),
  initiators = c(0, 220, 300, 399, 500, 456, 400, 278, 180, 35),
  year = rep(c(2011, 2012, 2013, 2014, 2015), 2)
  ) 

initiators <- data |> 
  ggplot(aes(x = year, y = initiators, group = drug, color = drug)) +
  geom_line(linewidth = 1) +
  labs(
    x = "Calendar Year",
    y = "# initiators",
    color = "Exposure"
    ) +
  scale_color_manual(values=c('orange','blue')) +
  theme_minimal(base_size = 10)

# treatment switch
data <- tibble::tibble(
  drug = c(rep("Drug A", 6), rep("Drug B", 6)),
  prop = c(4.1, 7.7, 17.7, 22.2, 24.4, 24.5, 4.6, 13.2, 25.1, 29, 29.3, 31.3),
  time = rep(seq(1, 12, 2), 2)
  ) 

switchers <- data |> 
  ggplot(aes(x = time, y = prop, group = drug, color = drug)) +
  geom_line(linewidth = 1) +
  labs(
    x = "Follow-up time [Months]",
    y = "Cumulative switch rate",
    color = "Exposure"
    ) +
  scale_color_manual(values=c('orange','blue')) +
  theme_minimal(base_size = 10)

# combine plots with shared legend
shared_legend <- get_legend(initiators)

plot_utilization <- suppressMessages(plot_grid(
  plot_grid(initiators + theme(legend.position = "top")),
  plot_grid(switchers + theme(legend.position = "none")),
  ncol = 1,
  nrow = 2,
  labels = c("a)", "b)"),
  shared_legend
  ))

# save
suppressMessages(ggsave(
  plot = plot_utilization,
  filename = "Figure_2_utlization.png",
  path = here("manuscript"),
  dpi = 800
  ))

# print to manuscript
include_graphics(path = here("manuscript", "Figure_2_utlization.png"))
```

{{< pagebreak >}}

```{r}
#| label: fig-balance
#| fig-cap: "Assessment of balance after propensity score matching or weighting across multiple imputed datasets."
#| fig-widh: 4
#| fig-height: 7

source(here("functions", "simulate_flaura.R"))

data <- simulate_flaura(
  n_total = 2000, 
  seed = 41, 
  include_id = FALSE, 
  imposeNA = TRUE,
  propNA = .25
  ) |> 
  mutate(treat = ifelse(treat == 1, "Drug A", "Drug B"))
 
# impute
mids_data <- mice(data, m = 10, printFlag = F)

# weight
covariates <- data |> 
  select(starts_with("dem_"), starts_with("c_"))

# ps formula
ps_formula <- as.formula(paste0("treat ~ ", paste0(colnames(covariates), collapse = " + ")))

# weighting  
wimids_data <- weightthem(
  datasets = mids_data, 
  formula = ps_formula, 
  method = "glm",
  estimand = "ATO"
  )

# Create the named character vector
# named_covariates <- setNames(paste("Covariate", seq_len(ncol(covariates))), colnames(covariates))

# covariate balance
covariate_balance <- love.plot(
  x = wimids_data,
  abs = TRUE,
  var.names = named_covariates,
  thresholds = 0.1, 
  drop.distance = TRUE,
  var.order = "unadjusted",
  colors = c("orange", "blue"), 
  stars = "std",
  shapes = 17, 
  size = 4, 
  grid = TRUE,
  position = "top"
  )

# distributional balance
score_balance <- bal.plot(
  x = wimids_data,
  var.name = "prop.score",
  which = "both",
  which.imp = .none,
  colors = c("orange", "blue")
  ) +
  labs(
    x = "Prognostic score",
    title = "Distributional balance for prognostic score"
    ) +
  theme(legend.position = "top")

plot_balance <- suppressMessages(plot_grid(
  plot_grid(covariate_balance),
  plot_grid(score_balance),
  ncol = 2,
  nrow = 1,
  labels = c("a)", "b)"),
  shared_legend
  ))

# save
suppressMessages(ggsave(
  plot = plot_balance,
  filename = "Figure_3_balance.png",
  path = here("manuscript"),
  dpi = 800,
  width = 15
  ))

# print to manuscript
include_graphics(path = here("manuscript", "Figure_3_balance.png"))
```
