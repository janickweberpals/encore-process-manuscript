---
title: "Emulating Comparative Oncology Trials with Real-world Evidence Studies (ENCORE): Process Development and Methodological Considerations for Oncology Real-World Data"

code-fold: false
echo: false
number-sections: false
warnings: false
messages: false

format: 
  docx:
    fig-cap-location: top
    tbl-cap-location: top
    reference-doc: custom-reference-doc.docx
  pdf:
    fig-cap-location: top
    tbl-cap-location: top
    extra_dependencies: ["float"]
    keep-tex: true
    fig-pos: 'h'
    tbl-pos: 'h'
    
editor: visual
bibliography: references.bib
csl: clinical-pharmacology-and-therapeutics.csl

filters:
  - docx-landscape.lua
---

```{r}
#| label: setup
#| include: false

library(here)
library(dplyr)
library(knitr)
library(gt)
library(readxl)
library(fs)
library(webshot2)
library(tibble)
library(ggplot2)
library(cowplot)
library(mice)
library(MatchThem)
library(cobalt)
library(gtExtras)
library(glue)
library(tidyr)
```

**Authors**: Janick Weberpals^1^, Sebastian Schneeweiss^1^, Kenneth L. Kehl^2^, Donna R. Rivera^3\*^, Pallavi Mishra-Kalyani^3^, Catherine C. Lerro^3\*^, Erin Larkins^4^, Preeti Narayan^4^, Richard Curley^3^, Georg Hahn^1^, Priyanka Anand^1^, Yanina Natanzon^5^, Andrew J. Belli^6^, Ching-Kun Wang^6^, Jenna Collins^7^**,** Jonathan Kish^7^**,** Janet Espirito^8^, Nicholas J Robert^8^, Robert J. Glynn^1^, Shirley V. Wang^1^

[Author affiliations:]{.underline}

^1^ Division of Pharmacoepidemiology and Pharmacoeconomics, Department of Medicine, Brigham and Women's Hospital, Harvard Medical School, Boston, MA, USA

^2^ Dana-Farber Cancer Institute, Boston, MA, USA

^3^ Oncology Center of Excellence, US Food and Drug Administration, Silver Spring, MD, USA

^4^ Center for Drug Evaluation and Research, US Food and Drug Administration, Silver Spring, MD, USA

^5^ ConcertAI, Cambridge, MA, USA

^6^ COTA, Inc., New York, NY, USA

^7^ Flatiron Health, Inc., New York, NY, USA

^8^ Ontada, Boston, MA, USA

\*At the time this research was conducted

[**Correspondence:**]{.underline}

Shirley V. Wang, PhD

Division of Pharmacoepidemiology and Pharmacoeconomics,

Department of Medicine, Brigham and Women's Hospital, Harvard Medical School,

1620 Tremont Street, Suite 3030-R, Boston, MA 02120, USA

Phone: +1 617-278-0932

Fax: + 1 617-232-8602

Email: [SWANG1\@BWH.HARVARD.EDU](SWANG1@BWH.HARVARD.EDU)

[**Article type:**]{.underline} Review

[**Manuscript word count:**]{.underline} 4,541 words / 8,000 words

[**Abstract word count:**]{.underline} 224 words / 250 words

[**Tables:**]{.underline} 3

[**Figures:**]{.underline} 3

[**Supplementary material:**]{.underline} Supplementary figures, tables and material

[**Short running title**]{.underline}: Emulation of Comparative Oncology Trials with Real-world Evidence (ENCORE)

[**Keywords:**]{.underline} Oncology, Real-World Evidence, Trial emulation, EHR

[**FDA Disclaimer:**]{.underline}This article reflects the views of the authors and should not be construed to represent FDA’s views or policies.

[**Funding Statement:**]{.underline} This project was supported by the Oncology Center of Excellence, Food and Drug Administration (FDA) of the U.S. Department of Health and Human Services (HHS) as part of a contract \[75F40122C00181\]. The contents are those of the author(s) and do not necessarily represent the official views of, nor an endorsement by FDA/HHS, or the U.S. Government.

[**Competing Interests Statement:**]{.underline} Dr. Weberpals is now an employee of AstraZeneca and owns stocks in AstraZeneca. Dr. Kehl has received research funding from Meta, Inc. to his institution. Drs. Espirito and Robert are employees of McKesson and own McKesson stock. Dr. Wang has consulted ad hoc for Exponent Inc. and MITRE a federally funded research center for the Centers for Medicare and Medicaid Services on unrelated work. Dr. Glynn has received support for investigator-initiated grants to the Brigham and Women's Hospital from Amarin, AstraZeneca, Kowa, Novartis, and Pfizer unrelated to the current work. Dr. Schneeweiss is participating in investigator-initiated grants to the Brigham and Women's Hospital from Bayer and UCB unrelated to the topic of this study. He consults for and owns equity in Aetion Inc., a software manufacturer. He is an advisor to Temedica GmbH, a patient-oriented data generation company. His interests were declared, reviewed, and approved by the Brigham and Women's Hospital in accordance with their institutional compliance policies.

[**Data sharing statement:**]{.underline} No data was analyzed as part of this project.

[**Analytic code sharing statement:**]{.underline} Simulated examples and code to implement analytic workflows described in this manuscript are illustrated at <https://janickweberpals.github.io/imputation-ps-workflows/> and can be reproduced via the `encore.analytics` R package (<https://github.com/janickweberpals/encore.analytics/>).

*Manuscript last updated: `r Sys.time()`*

{{< pagebreak >}}

# Abstract {.unnumbered}

Real-world evidence (RWE) is increasingly used to complement findings from randomized controlled trials (RCTs), contextualizing the effectiveness and safety of medical interventions as delivered in routine clinical practice. Advances in the curation and accessibility of electronic health record data (EHR) present the opportunity to utilize real-world data (RWD) to investigate therapeutic areas including oncology, where administrative healthcare claims databases alone are often not fit-for-purpose. The RCT DUPLICATE initiative has previously evaluated when RWE can most appropriately draw causal conclusions by emulating trials for non-oncology indications. Here, we present the design and trial selection for the Emulation of Comparative Oncology Trials with Real-world Evidence (ENCORE) project, which extends this work to oncology. ENCORE is designed to emulate 12 RCTs in four oncology-specialized EHR databases across four different cancer indications, specifically non-small cell lung cancer, breast cancer, colorectal cancer, and multiple myeloma. It will place special emphasis on systematic evaluation of fitness of data in relation to the study design and statistical analysis for a particular research question, and pre-registration of study protocols prior to initiation and analysis. Pre-specified criteria will assess agreement of treatment effect estimates between RCTs and their respective emulations. Through extensive sensitivity analyses benchmarked against RCT results, the ENCORE project aims to inform understanding of how measurement, design, and analytic decisions influence the interpretation of results from emulated oncology trials using RWD.

{{< pagebreak >}}

# Background

Randomized controlled trials (RCTs) are the evidentiary gold standard methodology for establishing the efficacy and safety of medical products. Guided by the 21^st^ Century Cures Act,[@RWEFDA] the Food and Drug Administration (FDA) established a framework to factor considerations for use of real-world evidence (RWE) generated from real-world data (RWD) such as electronic health records (EHR) to support approval of new indications or satisfy postmarketing requirements.[@RWEFDA] Accounting for approximately 30% of new FDA drug approvals, oncology was the disease area with the most new approvals in 2023[@mullard20242023] as well as several indication expansions, and has many areas of high unmet medical need to treat serious conditions. Therefore, RWE can have a particularly important potential to complement evidence from RCTs in the field of oncology. Potential uses include the assessment of effectiveness in specific patient populations that are not adequately represented in RCTs, or the precision oncology-focused discovery of biomarkers among pan-tumor populations that harbor specific genomic and immuno-pathological signatures.

However, the comparability and transportability of results derived between non-interventional studies and RCTs depends on multiple factors. Frequently referenced limitations include lack of baseline randomization and the imbalance in prognostic factors resulting thereof, missing data, unmeasured confounding, small study sizes, data discontinuity,[@Merola2022; @joshua2022longitudinal] adoption of changes in treatment guidelines over time in routine clinical care and the inability to measure and emulate common eligibility criteria, including prognostic factors, and standardized endpoint assessments in real-world data (RWD).[@rider2024emulations] While examples of oncology trial emulations have been published,[@rider2024emulations; @merola2023aetion; @merola2024calibrating] a systematic and scaled approach to emulate a diverse set of oncology trials with multiple heterogeneous databases would enhance increase confidence in the interpretability of non-interventional studies, evaluate regulatory considerations, and to provide context as to which questions can be validly answered.

The RCT DUPLICATE initiative[@wang2023emulation] evaluated when non-interventional studies can come to causal conclusions on treatment effects by benchmarking results against RCTs under the assumption that well-designed and conducted RCT findings reflect causal treatment effects. In settings where the RCT designs could be emulated well, non-interventional studies were able to reach similar conclusions.[@heyard2024design] However, this prior work from RCT-DUPLICATE focused on emulating trials in non-oncology settings using claims databases.

The *Emulation of Comparative Oncology Trials with Real-world Evidence* (ENCORE) project [@encoreFDA] aims to extend this work to oncology. Clinical studies in oncology come with unique methodological challenges which must be systematically explored and understood. Building on a process co-developed with the FDA through RCT DUPLICATE,[@wang2023emulation] this expansion to oncology will emulate 12 randomized clinical trials using multiple specialty oncology EHR data sources. The process will emphasize transparency and include a data fitness-for-use assessment[@rivera2024oncology; @gatto2022structured] for each RWD source with respect to each trial emulation as well as extensive sensitivity analyses to assess robustness of findings.

The objectives of this project are to 1) develop state-of-the-art methodological approaches and 2) apply these methods to gain insights into the potential use of RWE to enhance regulatory and clinical decision-making in oncology. To achieve these objectives, this demonstration project will systematically emulate 12 oncology trials across four cancer types and assess the agreement of treatment effect estimates between RCTs and their respective emulations.

Here, we describe the design and process for the selection of the 12 oncology RCTs, assessment of the database quality and selection, protocol development and pre-registration, study design and statistical analysis plans, and pre-specified agreement metrics to evaluate the concordance between RCTs and emulations.

# Methods

A visual summary of the entire systematic process from trial selection to final results is provided in @fig-process.

## Trial selection

The focus of ENCORE is to clarify when non-interventional studies can or cannot yield similar results compared to oncology RCTs. Therefore, the project emphasizes trials of therapies for common cancers for which there has been substantial therapeutic development in recent years. After review and collaboration with clinical and regulatory experts, four cancer types were identified: non-small cell lung cancer, breast cancer, colorectal cancer and multiple myeloma. For each cancer type, we aim to conduct three trial emulations using multiple accessible databases (i.e., the total equaling 12 trial emulations x *n* databases which are found fit-for-purpose for each trial).

We used a systematic process for trial selection where the eligibility criteria are documented in CONSORT diagrams showing reasons for excluding RCTs by cancer type. The search was conducted using the *Aggregated Analysis of ClinicalTrials.gov* (AACT) database which is a publicly available relational database developed and maintained by the Clinical Trials Transformation Initiative (CTTI) that contains information (protocol and result data elements) about studies registered on ClinicalTrials.gov.[@tasneem2012database] To identify eligible trials, we used a combined search query strategy of the National Library of Medicine (NLM)-controlled *Medical Subject Headings **(***MeSH) term and a free keyword search for the respective cancer indication in the *conditions*, *studies* and *detailed_descriptions* fields of each trial entry on ClinicalTrials.gov.

Eligible trials had to fulfill the following basic criteria:

-   Interventional

-   Randomized

-   Intervention model: Parallel assignment

-   Industry-sponsored

-   Trial start: 2011 or later

-   Primary purpose: Study treatment efficacy

-   Endpoint(s): Overall survival must be one of the endpoints reported (either as hazard ratio or median overall survival time)

-   Recruitment status: ‘Completed’ or ‘Active, not recruiting’

-   Feasibility and clinical relevance (latter was defined as treatment or paradigm-changing trials or trials that challenged existing treatment policies)

The operationalization for each criterion is listed in detail in @tbl-criteria. We mainly considered pivotal large late-phase RCTs that were initiated after 2011 because treatment guidelines for included cancer indications have undergone significant changes in recent years. Due to the rapid adoption of newly-approved therapies in routine care, patients are less likely to receive outdated treatment regimens in clinical practice. Conversely, we excluded trials with results published too recently to allow for substantive medical product uptake and follow-up time accrual in real-world databases used for this project. We did not define a global cut-off as the requirements for follow-up time are different for each cancer type and population (e.g., advanced NSCLC versus early-stage breast cancer) and the decisions were made on a clinically relevant basis.

Although there have been substantial methodological advancements to increase our understanding on the emulation and comparison of real-world progression-free survival (PFS) and objective response rates (ORR) to a RECISTv1.1[@eisenhauer2009new]-based PFS and ORR assessment in RCTs [@ThanCCR; @mckelvey2024evaluation], imaging-based evaluations still require a level of granularity which is not well reflected in chart-abstracted assessments of a patient's progression in routine care.[@rwdRECIST; @rivera2022friends] The timing and cadence of intervals between progression assessments can differ between RCTs and routine care which may result in measurement error and bias.[@ackerman2024measurement] Given this, and the large number of other methodological challenges including missing data, small sample sizes, data discontinuity and rapidly changing guideline treatments, we focus on the emulation of overall survival (OS) as the endpoint of interest. Therefore, we include only trials that have reported OS as a pre-specified endpoint in the protocol.

While most trial eligibility criteria could be operationalized in an automated fashion, our final criterion was an assessment of emulation feasibility and clinical relevance. This criterion involved extensive human review. The fundamental points considered in this step include an initial feasibility assessment of the data fitness,[@rivera2022friends] involving an assessment of whether critical eligibility criteria (e.g., biomarker status) including prognostic factors (e.g., Eastern Cooperative Oncology Group \[ECOG\] performance score) are measurable, and whether preliminary study size based on a rough estimation of the number of patients observed in the data with the combination of treatment regimen and line of therapy is reasonable for shortlisting the trial. Lastly, trial candidates were ranked and shortlisted as primary or runner-up based on their clinical and regulatory relevance.

A list of tentative, shortlisted primary candidates is presented in @tbl-rcts. The corresponding selection process is illustrated in the CONSORT diagrams (**Supplementary Figures 1-4**) and runner up candidate trials are listed in **Supplementary Table 1**. The majority of shortlisted trials represent indications for advanced (locally advanced, inoperable, recurrent/progressive disease) or metastatic cancer patients because a large proportion of drug development efforts have recently focused on these settings. A key objective that we aim to explore with the selected trials is to evaluate how stage (early, late), line of therapy (\[neo\]adjuvant, first line, advanced lines of therapy), therapy protocols (monotherapy, combination therapy) and clinical characteristics (all-comer versus complex genetic or immunological signatures) can be emulated using RWD. If more thorough feasibility assessments after applying all relevant eligibility criteria (including formal power calculations and assessment of sufficient balance after propensity score matching or weighting) suggest that the threat of bias from mis-measurement of key study parameters or residual confounding remains high, or that the study size is not sufficient, then runner-up candidate trials will be considered instead.

## Databases

The ENCORE project will use data from four U.S.-based oncology-specific EHR-derived RWD data sources (in alphabetical order): ConcertAI, COTA, Flatiron Health, and Ontada/McKesson. A detailed description and sampling methodology will be provided with each trial emulation protocol. For ENCORE, not all databases will be available for each cancer indication given the specificity of data elements required and the names of the databases will be blinded and referred to as ENCORE DataBase (EDB) 1, 2, 3 and 4 for the final reporting of results (in randomly assigned order). If more than one database is considered fit-for-use for a respective trial emulation, the most suitable analytic model will be employed for each database separately and final treatment effect estimates will be pooled using a meta-analytic approach using random effects (primary; weights reflect the inverse of the variance of each database study plus an additional variance term that quantifies the assumed heterogeneity between databases) and fixed effects (secondary; weights reflect the inverse of the variance of each database study) models.[@nikolakopoulou2014interpret]

<!--# maybe data partners have  1-2 sentences and/or publications to reference? -->

## Protocol development

For each selected RCT, a detailed protocol, pre-specifying key elements of the trial emulation, will be developed using the HARPER protocol template[@wang2022harmonized; @guidelinegeneral] and will be registered on ClinicalTrials.gov after review by an expert panel comprising clinicians and FDA reveiwers. Following the target trial emulation framework, we will provide an explicit description and rationale on how each element will be emulated including database selection, covariate measurement, operationalization of key eligibility criteria, study design, data analysis and causal contrasts of interest.[@hernan2022target; @hernan2016specifying] Since it is common that oncology RCTs update survival estimates periodically based on accrued follow-up time, the protocol will give a brief summary of each emulated RCT and specify which target OS estimates will be used for agreement metric evaluation (see @sec-agreement-metrics). All eligibility criteria will be extracted based on publications, publicly available protocols, and statistical analysis plans of the selected RCT.

**Emulating the RCT estimand**

An important aspect when emulating oncology trials is the choice and estimation of the appropriate estimand of interest.[@rufibach2018] In prior RCT-DUPLICATE efforts, a “while on treatment” strategy was chosen for database studies that were designed to emulate trials where the primary trial analysis was intention-to-treat (e.g., a “treatment policy” estimand). This estimand was chosen to mimic the high adherence typically observed in large cardiovascular clinical trials in the context of low adherence in clinical practice. However, a “while on treatment” estimand would be inappropriate when emulating oncology trials, not only because of highly informative censoring, but also because OS outcome analyses in oncology are typically “treatment policy” estimands that allow for crossover or other post-progression antineoplastic therapy.[@manitz2022estimands] We plan to focus on “treatment policy” estimands for the primary analyses in our trial emulations. However, allowing crossover can dilute treatment effects, moving point estimates toward the null. Therefore, differences in cross-over and discontinuation rates between the trial and emulation may still contribute to differences in observed outcomes. Recognizing these practical challenges, we will carefully characterize the patterns of discontinuation or cross-over in both the trials and the database studies designed to emulate them.

### Emulation feasibility

#### **Fit-for-purpose data**

Real-world data fitness and emulation feasibility for each candidate trial will be assessed in multiple steps based on the approach described by the Oncology Quality, Characterization, and Assessment of Real-World Data (Oncology QCARD) Initiative.[@rivera2022friends] The first step assesses if relevant variables like exposure, line of therapy, outcomes, and covariates are generally available, measured and operationalizable in routine-care. Since many oncology RCTs in recent years have focused on biomarker-defined populations, nuances in measurement and operationalizability of specific biomarkers must be understood to ensure a representative and adequately sized study population. For example, immune checkpoint inhibitors have significantly changed the cancer treatment landscape since the first approvals of the CTLA-4 inhibitor ipilimumab in 2011 and the PD-1 inhibitors pembrolizumab and nivolumab in 2014 in the United States. However, the operationalization of the expression of the PD-L1 biomarker in RCTs (e.g., percent staining, tumor proportion score, or combined positive score) has evolved, and as such PD-L1 '*positivity'* may have different definitions by year based on different cut-off values.

According to the *Structured Process to Identify Fit-For-Purpose Data* (SPIFD) framework[@gatto2022structured], the next step outlines how eligibility criteria will be ascertained using a color-coded heatmap that will indicate the level of confidence on how well each criterion can be emulated in each selected database. As there are eligibility criteria in oncology clinical trials which either are infeasible to emulate (e.g., physician-assessed survival prognosis quantification) or that do not impact clinical care for the emulation of the trial (e.g., male patients should be willing to use barrier contraception), the study team will determine key eligibility criteria for the emulation of the trial based on consensus.

Additionally, we will provide conceptual and operationalized definitions on how exposures, outcomes and covariates are defined in each respective database. There will be special emphasis on how exposure, in context of disease and line of therapy settings, and the survival outcomes are emulated. For all considered databases, real-world OS (rwOS) is typically a composite measure that, depending on the underlying database, can be derived from different mortality sources (i.e., EHR documentation, Social Security Death Index, and other linkages). Given that not all relevant sources that provide mortality data are synchronized and updated uniformly, sensitivity analyses with more conservative (i.e., earlier) censoring dates will be considered for each trial emulation to mitigate the potential impact of ghost-time bias.[@meyer2020open]

#### **Descriptives and data exploration**

Particularly when emulating pivotal trials of practice-changing treatments, multiple aspects need to be considered, such as the contemporaneity of the control cohort, the adoption rate of the novel medical product in routine care, the magnitude of the clinical treatment benefit, and the rate in which patients discontinue or cross-over, as they could influence effect estimates.

To that end, comprehensive data explorations will be performed as part of the protocol development to contextualize these parameters and (if reported) evaluate comparisons to the emulated trial. Examples for such standard diagnostics are visualized in @fig-initiators.

The distribution of patient characteristics, by exposure status, will be examined before and after applying eligibility criteria and contrasted with the distributions of patient characteristics of the original RCT. Initial propensity score matching or weighting methods will be applied to ensure that measured pre-exposure covariates can be balanced[@austin2009balance], exposure cohorts are conditionally exchangeable at baseline, and resulting sample sizes are still sufficient after matching or weighting. At this stage, all exploratory analyses will be conducted blinded towards the outcome to not bias any study design or analytic choices based on known outcome information.

#### **Statistical power considerations**

Causal analyses of non-interventional data are often not designed with respect to formal hypothesis testing and statistical power in the same manner as RCTs since the number of 'recruited' patients is limited by available data .[@hernan2022causal] For this project, however, the emulations in EHR data must have at least equal power to the relevant trial for interpretation of the pre-specified agreement metrics between RCT and RWD results. Since the main outcome of interest is defined as time from index treatment initiation to all-cause mortality (rwOS), the estimation of the statistical power is driven by the number of events rather than the number of patients. To assess whether the overall number of events, unstratified by exposure, is sufficient such that a significant difference can be detected based on the original RCT-reported hazard ratio (HR), the statistical power (1-$\beta$ with a 2-sided $\alpha$ of 0.05) will be estimated using Schoenfeld's sample-size formula for the proportional-hazards regression model.[@schoenfeld1981asymptotic]

<!--# https://github.com/keaven/gsDesign/blob/master/R/nEvents.R#L22C12-L22C78 -->

## Agreement metrics {#sec-agreement-metrics}

To formally compare treatment effects between RCTs and their respective emulations, we will adapt the approach of the RCT-DUPLICATE project.[@franklin2020nonrandomized; @wang2023emulation] That is, for the primary endpoint of interest (hazard ratio \[HR\] for OS and corresponding 95% confidence intervals), we will derive three qualitative agreement metrics: statistical agreement, estimate agreement and agreement based on the standardized mean difference (SMD). Examples are illustrated in @tbl-metrics.

**Statistical significance agreement**: agreement between RCT and emulated trial treatment effect with regards to directionality and statistical significance (nominal in the case of emulated trials).

**Estimate agreement**: agreement that the estimated RWE treatment effect is within the 95% CI of the RCT treatment effect estimate. Provided that for some emulations, the power of the RWE study may be larger than that of the original RCT, this could lead to situations where there is no statistical significance agreement (RCT estimate crossing the null although the treatment effect estimates are overlapping) or vice versa in case the RCT has a larger power than the RWE emulation.

**SMD agreement**: quantification of the agreement between the emulated RWE and RCT treatment effect estimate. The SMD is calculated as

$$
SMD = \frac{\hat{\theta}_{RCT} - \hat{\theta}_{RWE}}{\sqrt{\text{Var}(\hat{\theta}_{RCT}) + \text{Var}(\hat{\theta}_{RWE})}}
$$

where $\hat{\theta}_{RCT}$ and $\hat{\theta}_{RWE}$ are the treatment effect estimates (log hazard ratios or median survival time differences) and $\text{Var}(\hat{\theta}_{RCT})$ and $\text{Var}(\hat{\theta}_{RWE})$ are the corresponding variances for RCT and RWE, respectively. The resulting SMDs will be interpreted such that with an SMD of 1.00, the effect estimate from the RCT and the RWE emulation are 1 standard deviation apart. For an $\alpha$-level of 0.05, the null hypothesis of no difference would be rejected whenever $|Z| > 1.96$.

For the secondary endpoints of interest (e.g., median survival time or survival probabilities) only the SMD agreement metric will be applicable.

## Study design and statistical analysis

The study design for each trial emulation will be visualized as part of the protocol using a graphical depiction of the exact measurement windows of eligibility criteria, washout periods, and covariates relative to the cohort entry time.[@schneeweiss2019graphical]

### Missing data

To establish an analytic cohort, missingness will be assessed across patients who meet all eligibility criteria, including those with missing values as a first iteration. These missing data investigations will empirically assess assumptions on potential underlying missingness mechanisms according to Rubin’s classification of missing data (i.e., missing completely at random \[MCAR\], missing at random \[MAR\] and missing not at random \[MNAR\]).[@rubin1976inference] We will adopt a principled process on missing data that empirically evaluates different aspects across partially observed covariates based on three group diagnostics.[@weberpals2024smdi; @weberpals2024] These diagnostics cover (1) comparisons of patients characteristics with and without an observed level of the partially observed covariate, (2) ability to predict missingness given observed data, and (3) assessments if outcomes between patients with a missing value are systematically different. Expert domain knowledge and assumptions about the underlying missing data structure through canonical causal diagrams[@moreno2018canonical] will additionally inform decisions regarding the inclusion or exclusion of individuals with missing values in key eligibility criteria and potential sensitivity analyses to assess the robustness of these decisions.[@tompsett2018use]

While MAR is a strong assumption to hold across all considered covariates, it has been shown that especially in the context of partially observed covariate data (as opposed to missing exposure and outcome data), generally only mechanisms in which a covariate causes its own missingness lead to significant bias (MNAR).[@moreno2018canonical] Hence, methodologies which retain otherwise eligible patients and give the potential to adjust for a broader set of prognostic factors (e.g., multiple imputation[@weberpals2024hdmi] or doubly robust methods[@Shaw2024]) may be preferred over complete case analyses, although such tradeoffs must be carefully evaluated within the clinical and study contexts.

### Outcome and propensity score analyses

Due to its frequency of use in oncology trials, the primary parameter of interest in ENCORE will be defined as the marginal HR coefficient for the treatment comparison for OS.[@cox1972regression] We will also consider alternative endpoints on an absolute risk scale such as median survival times, survival probabilities at pre-defined time points during follow-up[@kaplan1958nonparametric] and restricted mean survival times as secondary endpoints of interest.

For the estimation of marginal treatment effects, we will employ propensity score methods to adjust for measured confounding between treatment arms. The selection of relevant prognostic covariates will be based on expert clinical knowledge and published literature on prognostic scores in oncology.[@becker2020enhanced] The implementation of propensity scores in combination with multiple imputation will follow the '*within*' methodology as described by Leyrat et al.[@leyrat2019propensity; @pishgar2020matchthem] As appropriate, propensity score matching or weighting will be applied to each imputed dataset. The marginal treatment effect will then be estimated in each imputed and matched or weighted dataset separately and pooled into a final estimate following Rubin's rule.[@rubin2018multiple; @van2011mice] This approach has been shown to lead to unbiased estimates across different simulated scenarios with a sufficient estimation of the variance.[@leyrat2019propensity]

To assess the balance of pre-exposure covariates after matching or weighting on each imputed dataset, the average SMD and corresponding minimum and maximum SMD range will be visualized (see example in @fig-balance). Covariate balance is often considered reasonable at a SMD \< 0.1.[@austin2009balance] Further, we will compute the average post-matching or post-weighting C-statistics.[@franklin2014metrics] In addition, we will use a published prognostic score for OS[@becker2020enhanced] as a balance measure to visually assess if the prognostic score is balanced between treatment arms after propensity score matching or weighting as this approach was described to show the highest correlations with bias compared with other balance measures and not affected by model misspecification.[@stuart2013prognostic]

Similarly, survival probabilities for individual time points will be estimated in each imputed and propensity score matched or weighted dataset according to the Kaplan-Meier method.[@kaplan1958nonparametric] Since survival probabilities typically do not follow normal distributions which are required to apply Rubin's rule, these will be transformed through a complementary log-log transformation $log(-log(1-pr(surv)))$ with $pr(surv)$ denoting the survival probability at a given follow-up time during.[@warwick28685; @morisot2015prostate] The transformed survival probabilities are then pooled across imputed datasets and individual time points following Rubin's rule and back-transformed via $1-exp(-exp(qbar))$ with $qbar$ denoting the pooled survival probability. The median survival time can be finally determined by extracting the time point during follow-up at which the survival probability drops below 0.5 for the first time.

### Sensitivity analyses

With the goal to better understand which factors could influence differences between RCT results and emulated trial results, it is appropriate to conduct a range of sensitivity analyses. This can comprise decisions on the considered databases, calendar time period, covariate measurement (e.g., measurement windows or trade-offs on sensitivity versus specificity in measurements), approaches to missing data, selection of covariates for imputation and propensity score models, and censoring decisions. All sensitivity analyses will be pre-specified in the study protocol and reported using appropriate visualizations such as forest plots.

### Reproducibility and consistency across emulation

To ensure a transparent, reproducible and consistent way of deriving analytic cohorts and performing statistical analyses across trial emulations, we developed two R packages. The first package, `encore.io,` streamlines the query of analytic cohorts and measurement of covariates and outcomes using parameterized functions. Due to the blinding of databases used for ENCORE, this package is not able to be publicly available; however a comprehensive and detailed documentation for each function are available in the Supplementary Material. A second open-source R package, `encore.analytics`, will then facilitate the complex multi-step workflows of multiple imputation, propensity score matching and weighting and estimating pooled marginal treatment effects and Kaplan-Meier curves along with other established statistical R packages.[@anesrake; @MatchThem; @survival; @encore.analytics; @gtsummary; @ggsurvfit] Simulated examples with code that illustrate the analytic workflows described in this manuscript are available in the online Supplementary Material at <https://janickweberpals.github.io/imputation-ps-workflows/> and can be reproduced via the `encore.analytics` R package (<https://github.com/janickweberpals/encore.analytics/>).

{{< pagebreak >}}

# Discussion

Building on an established approach developed by the RCT DUPLICATE project[@wang2023emulation; @franklin2020nonrandomized], the ENCORE project aims to emulate 12 oncology trials using four oncology EHR data sources to inform the potential use of RWD for regulatory purposes in oncology. The project focuses on four common cancers and will evaluate the agreement of treatment effect estimates between RCTs and their respective emulations. Historically, administrative claims databases have been the predominant data source used to evaluate medical product safety and effectiveness in the postmarketing setting. With increasing access to EHR data and availability of granular clinical data elements often necessary to gather fit-for-use data in oncology, as well as a maturing set of methodological approaches for causal inference using such data, the ENCORE project is one of the first of its kind to evaluate contexts where RWD may be reliably used to draw similar conclusions compared to RCTs in the field of oncology.

The RCT DUPLICATE initiative[@wang2023emulation; @franklin2020nonrandomized] identified multiple emulation challenges that may be similar in ENCORE. One particular aspect that we may not always be able to emulate is the exact distribution of patient characteristics of the trial population. This can be due to the lack of data granularity and comprehensiveness to emulate relevant eligibility criteria or the fact that large pivotal trials in oncology are typically conducted in multiple countries worldwide (all considered databases reflect the US only). This may impact the results of this trial emulation given that the distribution of genetic alterations and biomarkers, prognosis, and factors that drive heterogeneous treatment outcomes of certain cancers can differ between countries. For example, certain cancers (e.g., GI cancers) or certain genetic mutations (e.g., EGFR mutations in lung cancer) are much more prevalent in certain geographic regions and populations compared to the US. Additionally, treatment landscapes such as the preference for specific regimens or compounds, their use in earlier or later lines of therapies, and supportive care practices can differ between geographic regions, too.

Another common challenge in the emulation of oncology trials is the estimation of an "treatment policy"/"intention-to-treat" analogous estimand which is usually the primary estimand reported in oncology RCTs. Due to intercurrent events, such as non-adherence, crossover of a high proportion of patients from the control to the intervention arm, or differences in subsequent therapies there may be observed differences in outcomes between trial and emulation.[@manitz2022estimands] Although this is also a common challenge in the analysis of RCTs,[@rufibach2018] treatment in routine clinical practice might be less stringent compared to trials in terms of pre-specified dosing schedules, monitoring and surveillance.[@merola2024calibrating] In order to derive comparable estimands it is therefore crucial to understand and contextualize the proportion and timing of treatment switching and discontinuation in both the trial and its emulation.

This issue may be amplified with more complex treatment protocols like combination regimens (e.g., CheckMate9LA) as compared to monotherapies (e.g., CheckMate057) since the ascertainment of the exposure needs to happen over a pre-defined time window which is often difficult to calibrate in RWD. As a result, exposure misclassification (due to overly narrow ascertainment windows) and selection bias (due to overly long ascertainment windows) are common trade-offs in such scenarios. Alternative analytic approaches which target a per-protocol estimand, such as the clone-censor-weight design,[@gaber2024mystifying] may be viable options if the ITT estimand cannot be estimated due to the aforementioned parameters, provided that necessary covariate measurements are available to account for the artificial censoring introduced with these methods.

## Conclusions

Principled, well-designed and reproducible studies using fit-for-purpose RWD to generate RWE, such as through trial emulation, may complement evidence from RCTs. Through a systematic benchmarking approach, the ENCORE project will provide insights as to how methodological, design, and analytic decisions influence quantifiable bias and interpretability of non-interventional studies in oncology.

{{< pagebreak >}}

# References {.unnumbered}

::: {#refs}
:::

{{< pagebreak >}}

# Tables {.unnumbered}

```{r}
#| label: tbl-criteria
#| tbl-cap: "Criteria to select eligible trials for emulation in ENCORE."
#| tbl-cap-location: top
#| fig-pos: 'h'
#| tbl-pos: 'h'

# read excel blueprint
eligibility_criteria <- read_excel(
  path = here("tables", "helper_tables.xlsx"),
  sheet = "ctgov_criteria_defintions"
  )

# convert to gt table
tbl_eligibility_criteria <- eligibility_criteria |> 
  select(-Link) |> 
  filter(Criteria != "Study phase") |> 
  gt() |> 
  tab_style(
    style = cell_text(weight = "bold"),
      locations = cells_column_labels()
    )

# save table as .docx document
tbl_eligibility_criteria |> 
   gtsave(
    filename = here("manuscript", "Table_1_trial_eligibility.docx")
    )

# save image to display within manuscript document
suppressMessages(tbl_eligibility_criteria |> 
  gtsave(
    filename = here("tables", "Table_1_trial_eligibility.png"),
    expand = 10, # amount of whitespace,
    zoom = 5 # higher resolution
    ))

include_graphics(path = here("tables", "Table_1_trial_eligibility.png"))
```

{{< pagebreak >}}

```{r}
#| label: tbl-rcts
#| tbl-cap: "Tentative list of randomized controlled trials (RCTs) considered for emulation."
#| tbl-cap-location: top
#| fig-pos: 'h'
#| tbl-pos: 'h'

rct_selection <- read_excel(
  path = here("tables", "helper_tables.xlsx"),
  sheet = "rct_selection"
  )

tbl_rct_selection <- rct_selection |> 
  group_by(cancer) |> 
  gt() |> 
  cols_label(
    nctid = md("**NCTID**"),
    acronym = md("**Acronym**"),
    setting = md("**Clinical setting**"),
    line = md("**Line of therapy**"),
    exposures = md("**Treatment comparison**")
    ) |> 
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_row_groups()
    )

# save table as .docx document
tbl_rct_selection |> 
   gtsave(
    filename = here("manuscript", "Table_2_trial_selection.docx")
    )

# save image to display within manuscript document
suppressMessages(tbl_rct_selection |> 
  gtsave(
    filename = here("tables", "Table_2_trial_selection.png"),
    expand = 10, # amount of whitespace,
    zoom = 5 # higher resolution
    ))

include_graphics(path = here("tables", "Table_2_trial_selection.png"))
```

{{< pagebreak >}}

```{r}
#| label: tbl-metrics
#| tbl-cap: "Example visualization of agreement metrics."
#| fig-pos: 'h'
#| tbl-pos: 'h'

source(here("functions","smd_agreement.R"))

data <- tribble(
  ~trial, ~group, ~estimate, ~lower, ~upper,
  "Trial 1", "RCT", 0.75, 0.61, 0.91,
  "Trial 1", "RWE", 0.80, 0.51, 1.1,
  "Trial 2", "RCT", 0.62, 0.51, 0.71,
  "Trial 2", "RWE", 0.65, 0.55, 0.69,
  "Trial 3", "RCT", 0.71, 0.67, 0.80,
  "Trial 3", "RWE", 0.51, 0.41, 0.61,
  "Trial 4", "RCT", 0.90, 0.81, 0.99,
  "Trial 4", "RWE", 1.2, 1.09, 1.34,
  ) |> 
  pivot_wider(names_from = group, values_from = c(estimate, lower, upper)) |> 
  mutate(
    smd_value = smd_agreement(
      rct_estimate = log(estimate_RCT), 
      rct_lower = log(lower_RCT), 
      rct_upper = log(upper_RCT),
      rwe_estimate = log(estimate_RWE),
      rwe_lower = log(lower_RWE), 
      rwe_upper = log(upper_RWE)
      )
    ) |> 
  mutate(
    regulatory = ifelse(
      # superiority
      estimate_RCT < 1 & upper_RCT < 1 & estimate_RWE < 1 & upper_RWE < 1 |
        # non-inferiority
        estimate_RCT < 1 & upper_RCT >= 1 & estimate_RWE < 1 & upper_RWE >= 1 |
        # inferiority (made up)
        estimate_RCT >= 1 & lower_RCT >= 1 & estimate_RWE >= 1 & upper_RWE >= 1,
      "Yes", "No"
      ),
    estimate = ifelse(
      estimate_RWE >= lower_RCT & estimate_RWE <= upper_RCT, "Yes", "No"
      ),
    smd = ifelse(
      abs(smd_value) < 1.96, "Yes", "No"
      )
    )

tbl_metrics <- data |> 
  mutate(across(where(is.numeric), function(x) format(x, digits = 2, nsmall = 2))) |> 
  mutate(
    RCT = glue("{estimate_RCT} ({lower_RCT} - {upper_RCT})"),
    RWE = glue("{estimate_RWE} ({lower_RWE} - {upper_RWE})"),
    smd = glue("{smd} ({smd_value})")
  ) |> 
  select(trial, RCT, RWE, regulatory, estimate, smd) |> 
  gt() |> 
  tab_spanner(
    label = "HR (95% CI)",
    columns = c(RCT, RWE)
  ) |> 
  cols_label(
    trial = "Trial",
    regulatory = md("Statistical <br> significance <br> agreement"),
    estimate = md("Estimate <br> agreement"),
    smd = "SMD"
    ) |> 
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
    ) |> 
    tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_spanners()
    ) |> 
  tab_style(
    style = cell_text(color = "darkgreen"), 
    locations = cells_body(
      columns = regulatory,
      rows = regulatory == "Yes"
      )
    ) |> 
  tab_style(
    style = cell_text(color = "darkred"), 
    locations = cells_body(
      columns = regulatory,
      rows = regulatory == "No"
      )
    ) |> 
  tab_style(
    style = cell_text(color = "darkgreen"), 
    locations = cells_body(
      columns = estimate,
      rows = estimate == "Yes"
      )
    ) |> 
  tab_style(
    style = cell_text(color = "darkred"), 
    locations = cells_body(
      columns = estimate,
      rows = estimate == "No"
      )
    ) |> 
  tab_style(
    style = cell_text(color = "darkgreen"), 
    locations = cells_body(
      columns = smd,
      rows = stringr::str_detect(smd, "Yes")
      )
    ) |> 
  tab_style(
    style = cell_text(color = "darkred"), 
    locations = cells_body(
      columns = smd,
      rows = stringr::str_detect(smd, "No")
      )
    ) |> 
  tab_footnote("Abbreviations: CI = Confidence interval, HR = Hazard ratio, RCT = Randomized controlled trial, RWE = Real-world evidence, SMD = standardized mean difference (based on log hazard ratios)")

# save table as .docx document
tbl_metrics |> 
   gtsave(
    filename = here("manuscript", "Table_3_agreement_metrics.docx")
    )

# save image to display within manuscript document
suppressMessages(tbl_metrics |> 
  gtsave(
    filename = here("tables", "Table_3_agreement_metrics.png"),
    expand = 10, # amount of whitespace,
    zoom = 5 # higher resolution
    ))

include_graphics(path = here("tables", "Table_3_agreement_metrics.png"))
```

{{< pagebreak >}}

# Figures {.unnumbered}

```{r}
#| label: fig-process
#| fig-cap: "Systematic process to understand effectiveness claims of oncology trials using real-world evidence."
#| fig-cap-location: top
#| fig-width: 12
#| fig-pos: 'h'
#| tbl-pos: 'h'

knitr::include_graphics(here("figures", "process.png"))

# copy figure to manuscript
file_copy(
  path = here("figures", "process.png"),
  new_path = here("manuscript", "Figure_1_process.png"),
  overwrite = TRUE
  )
```

::: {.content-visible unless-format="pdf"}
[View figure in higher resolution here](https://github.com/janickweberpals/encore-process-manuscript/blob/main/manuscript/Figure_1_process.png)
:::

{{< pagebreak >}}

```{r}
#| label: fig-initiators
#| fig-cap: "Example visualization of descriptive drug utilization analyses displaying a) initiation trends between compared regimens based on calendar time, b) cumulative rate of patients switching to another line of treatment."
#| fig-widh: 2.5
#| fig-height: 5
#| fig-pos: 'h'
#| tbl-pos: 'h'

data <- tibble::tibble(
  drug = c(rep("Drug A", 5), rep("Drug B", 5)),
  initiators = c(0, 220, 300, 399, 500, 456, 400, 278, 180, 35),
  year = rep(c(2011, 2012, 2013, 2014, 2015), 2)
  ) 

initiators <- data |> 
  ggplot(aes(x = year, y = initiators, group = drug, color = drug)) +
  geom_line(linewidth = 1) +
  labs(
    x = "Calendar Year",
    y = "# initiators",
    color = "Exposure"
    ) +
  scale_color_manual(values=c('orange','blue')) +
  theme_minimal(base_size = 10)

# treatment switch
data <- tibble::tibble(
  drug = c(rep("Drug A", 6), rep("Drug B", 6)),
  prop = c(4.1, 7.7, 17.7, 22.2, 24.4, 24.5, 4.6, 13.2, 25.1, 29, 29.3, 31.3),
  time = rep(seq(1, 12, 2), 2)
  ) 

switchers <- data |> 
  ggplot(aes(x = time, y = prop, group = drug, color = drug)) +
  geom_line(linewidth = 1) +
  labs(
    x = "Follow-up time [Months]",
    y = "Cumulative switch rate",
    color = "Exposure"
    ) +
  scale_color_manual(values=c('orange','blue')) +
  theme_minimal(base_size = 10)

# combine plots with shared legend
suppressWarnings(shared_legend <- get_legend(initiators))

plot_utilization <- plot_grid(
  plot_grid(initiators + theme(legend.position = "top")),
  plot_grid(switchers + theme(legend.position = "none")),
  ncol = 1,
  nrow = 2,
  labels = c("a)", "b)"),
  shared_legend
  )

# save
suppressMessages(ggsave(
  plot = plot_utilization,
  filename = "Figure_2_utlization.png",
  path = here("manuscript"),
  dpi = 800
  ))

# print to manuscript
include_graphics(path = here("manuscript", "Figure_2_utlization.png"))
```

::: {.content-visible unless-format="pdf"}
[View figure in higher resolution here](https://github.com/janickweberpals/encore-process-manuscript/blob/main/manuscript/Figure_2_utlization.png)
:::

{{< pagebreak >}}

```{r}
#| label: fig-balance
#| fig-cap: "Assessment of a) covariate balance and b) distributional balance of a prognostic score for overall survival before and after propensity score matching or weighting across multiple imputed datasets."
#| fig-widh: 4
#| fig-height: 7
#| fig-pos: 'h'
#| tbl-pos: 'h'

source(here("functions", "simulate_data.R"))

data <- simulate_data(
  n_total = 2000, 
  seed = 41, 
  include_id = FALSE, 
  imposeNA = TRUE,
  propNA = .25
  ) |> 
  mutate(treat = ifelse(treat == 1, "Drug A", "Drug B"))
 
# impute
mids_data <- suppressWarnings(mice(data, m = 10, printFlag = F))

# weight
covariates <- data |> 
  select(starts_with("dem_"), starts_with("c_"))

# ps formula
ps_formula <- as.formula(paste0("treat ~ ", paste0(colnames(covariates), collapse = " + ")))

# weighting  
suppressMessages(wimids_data <- weightthem(
  datasets = mids_data, 
  formula = ps_formula, 
  method = "glm",
  estimand = "ATO"
  ))

# Create the named character vector
# named_covariates <- setNames(paste("Covariate", seq_len(ncol(covariates))), colnames(covariates))

# covariate balance
covariate_balance <- love.plot(
  x = wimids_data,
  abs = TRUE,
  thresholds = 0.1, 
  drop.distance = TRUE,
  var.order = "unadjusted",
  colors = c("orange", "blue"), 
  stars = "std",
  shapes = 17, 
  size = 4, 
  grid = TRUE,
  position = "top"
  )

# distributional balance
# Simulate data
set.seed(42)  # For reproducibility
n <- 2000  # Number of samples per group
drugA_pre <- rnorm(n, mean = 0, sd = 2.5 / 2)  # Spread = 2.5
drugB_pre <- rnorm(n, mean = -1, sd = 2.5 / 2)  # Spread = 2.5
drugA_post <- rnorm(n, mean = 0, sd = 2.5 / 2)  # Spread = 2.5
drugB_post <- rnorm(n, mean = 0, sd = 2.5 / 2)  # Spread = 2.5

# Combine into a data frame
data <- data.frame(
  Value = c(drugA_pre, drugA_post, drugB_pre, drugB_post),
  Group = rep(c("Drug A", "Drug A", "Drug B", "Drug B"), each = n),
  PrePost = rep(c("Unadjusted sample", "Adjusted sample", "Unadjusted sample", "Adjusted sample"), each = n)
  ) |> 
  mutate(PrePost = factor(PrePost, levels = c("Unadjusted sample", "Adjusted sample")))

# Create the plot
score_balance <- ggplot(data, aes(x = Value, fill = Group)) +
  geom_density(alpha = 0.5) +
  scale_fill_manual(values = c("orange", "blue")) +
  labs(
    title = "Distributional balance for prognostic score",
    x = "Value",
    y = "Density",
    fill = "Group"
  ) +
  theme_minimal() +
  theme(legend.position = "top") +
  facet_wrap(~PrePost)

plot_balance <- suppressMessages(plot_grid(
  plot_grid(covariate_balance),
  plot_grid(score_balance),
  ncol = 2,
  nrow = 1,
  labels = c("a)", "b)"),
  shared_legend
  ))

# save
suppressMessages(ggsave(
  plot = plot_balance,
  filename = "Figure_3_balance.png",
  path = here("manuscript"),
  dpi = 800,
  width = 15,
  height = 8
  ))

# print to manuscript
include_graphics(path = here("manuscript", "Figure_3_balance.png"))
```

::: {.content-visible unless-format="pdf"}
[View figure in higher resolution here](https://github.com/janickweberpals/encore-process-manuscript/blob/main/manuscript/Figure_3_balance.png)
:::
